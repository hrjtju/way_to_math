# 第二章 概率论速成
## 2.1 基本定义
让我们从一个谜题开始。

### 2.1.1 Bertrand 悖论
> **问题 2.1（Bertrand 悖论）** 
> 假如平面上有一个长为 $2$ 的的圆，然后我们 <font color="purple">随机地</font> 选一条弦。与以该圆同圆心，半径为 $1$ 的圆相交的概率是多少？

* 解法 1：由于每条弦由其中点唯一确定，因此与小圆相交的概率为小圆面积除以大圆面积，即 $\displaystyle \frac{1}{4}$.
* 解法 2：由于圆的旋转对称性，对于每一条弦，我们都能旋转整个图形，使得该弦是垂直的。大圆直径为 $4$，小圆直径为 $2$，弦与小圆相交，那它一定与（旋转后的）小圆直径相交（且垂直），因此概率为小圆直径除以大圆直径，即 $\displaystyle\frac{1}{2}$.
* 解法 3：由于圆的旋转对称性，我们总能旋转整个图形，使得它的一个端点位于大圆的最左侧。考虑弦与大圆水平直径的夹角 $\theta$，可知它落在 $\displaystyle\left[ -\frac{\pi}{2}, \frac{\pi}{2} \right]$ 之间，由几何关系可得，弦与小圆相交时，$\theta$ 落在 $\displaystyle \left[ -\frac{\pi}{6}, \frac{\pi}{6} \right]$ 之间。因此概率为这两个区间长度之比，也即 $\displaystyle \frac{1}{3}$
![600](Pasted%20image%2020250601153030.png)

真是离谱到家了，同样的问题居然有三个不同的答案！

### 2.1.2 概率空间
一个很自然的疑问就是，上面到底出了什么问题？（如果上面没问题，那数学大厦就要塌了）注意到我把”随机地“三个字用颜色标出来了，问题就出在这里：我们没有良好的定义这里的随机到底是”怎么随机“的。为了解决这样的问题，我们引入概率空间的概念。

我们先有一个非空集合 $\Omega$，其中的子集我们称之为”事件“。

> **定义 2.2（$\sigma$-代数）** 
> 一个 $\sigma$ 代数指的是非空集合 $\Omega$ 上的一个子集族 $\mathcal{U}$，并满足下面的条件
> * $\varnothing, \Omega \in \mathcal{U}$
> * 如果 $A \in \mathcal{U}$，那么其补集 $A^{c} \in \mathcal{U}$
> * 如果一列集合 $A_{1}, \dots \in \mathcal{U}$，那么有 $$\displaystyle \bigcup_{k=1}^{\infty} A_{k},  \bigcap_{k=1}^{\infty} A_{k} \in \mathcal{U}.$$

> **定义 2.3 （概率测度）** 
> 设 $\mathcal{U}$ 是 $\Omega$ 上的一个 $\sigma$-代数，我们称 $$P: \mathcal{U} \rightarrow [0, 1]$$是一个概率测度，如果它满足下面的条件
> 1. $P(\varnothing) = 0$, $P(\Omega) = 1$
> 2. 如果一列集合 $A_{1}, \dots \in \mathcal{U}$，则 $$P\left[\bigcup_{k=1}^{\infty} A_{k} \right] \leqslant \sum\limits_{k=1}^{\infty}P(A_{k}) $$
> 3. 如果一列集合  $A_{1}, \dots \in \mathcal{U}$ 互不相交，则 $$P\left[\bigcup_{k=1}^{\infty} A_{k} \right] = \sum\limits_{k=1}^{\infty}P(A_{k}) $$

从这个定义我们得到：假如 $A \subset B$，就有 $P(A) \leqslant P(B)$，因为 $P(B) = P(B) + P(B - A) \geqslant P(A)$。

> **定义 2.4（概率空间）** 
> $\Omega$ 是一个非空集合，$\mathcal{U}$ 是其上的一个 $\sigma$-代数，$P$ 是 $\mathcal{U}$ 上的概率测度；我们称三元组 $(\Omega, \mathcal{U}, P)$ 是一个概率空间。

> **术语约定 2.5**
> 1. 集合 $A \in \mathcal{U}$ 被称为一个**事件**，$\omega \in \Omega$ 被称为**样本点**
> 2. $P(A)$ 是事件 $A$ 的**概率**
> 3. 如果一个属性，对除了概率为零得事件以外均为真，我们称其为 **几乎必然成立 (almost surely, 简写为 a.s.)**。

概率空间是概率论的必要设定，在讨论或解决任何问题之前，我们都要明确它所指的概率空间是什么。现在回头看之前的 Bertrand 悖论，我们不难发现三个解法对悖论中的”随机“作出了不同的解读，对应着三个不同的概率空间。

下面给出一些典型的概率空间的示例。

> **示例 2.6 （Borel $\sigma$-代数、概率密度）**
> 包含了 $\mathbb{R}^{n}$ 上所有开集的 $\sigma$-代数称为 **Borel $\sigma$-代数**，记为 $\mathcal{B}$。假设 $f$ 是非负的，并有 $\displaystyle \int_{\mathbb{R}^{n}} f\,\mathrm{d}x = 1$。我们可以对任意的 $B \in \mathcal{B}$ 定义 $$P(B) = \int_{B} f(x) \, \mathrm d{x} $$于是 $(\mathbb{R}^{n}, \mathcal{B}, P)$ 是一个概率空间，$f$ 为概率测度 $P$ 的概率密度。

> **示例 2.7 （Dirac 测度）**
> 选定 $\mathbb{R}^{n}$ 中的一个点 $x_{0}$，对任意的 $B \in \mathcal{B}$，定义 $$P(B) \coloneqq \begin{cases}1 & \text{if }x_{0} \in B \\0  & \text{if }x_{0} \notin B\end{cases}$$则 $(\mathbb{R}^{n}, \mathcal{B}, P)$ 是一个概率空间，我们称 $P$ 为集中在 $x_{0}$ 处的 **Dirac 点质量**，并将其记作 $\delta_{x_{0}}$。

> **示例 2.8 （Buffon 投针问题）**
> 现在我们要向一个平面上随机地投长度为 $1$ 的针。平面上有一簇间隔距离为 $2$ 的平行直线，求针投下后与其中一条平行线相交的概率。首先我们要找一个概率空间。记 $h$ 为投下的针的中心到最近一条平行线的距离，$\displaystyle \theta < \frac{\pi}{2}$ 为针所在直线与平行线的夹角。于是我们有 $\displaystyle \Omega = \left[0, \frac{\pi}{2}\right) \times [0, 1]$，其中前者对应 $\theta$ 的取值，后者对应 $h$ 的取值。接下来令 $\mathcal{U}$ 是 $\Omega$ 的 Borel 子集，并令概率测度 $P$ 为 $$P(B) = \frac{2(\text{area of }B)}{\pi}\quad \text{for each }B \in \mathcal{U},$$这其实就是Borel子集与 $\Omega$ 相交的**大小**与 $\Omega$ 本身大小的比值。接下来我们记针与平行线相交为事件 $A$，由几何关系可知此时 $\displaystyle \frac{h}{\sin \theta} \leqslant \frac{1}{2}$。于是就有 $\displaystyle A = \left\{  (\theta, h) \in \Omega: h \leqslant \sin \frac{\theta}{2}  \right\}$，这样我们就能求得事件 $A$ 的概率 $$P(A) = \frac{2(\text{area of }A)}{\pi} = \frac{2}{\pi}\int_{0}^{\pi/2} {\frac{1}{2}\sin \theta} \, \mathrm d{\theta} = \frac{1}{\pi}. $$


### 2.1.3 随机变量

> **定义 2.9 （随机变量，$\mathcal{U}$-可测）** 设 $(\Omega, \mathcal{U}, P)$ 是一个概率空间，一个映射 $$\boldsymbol{X}: \Omega \rightarrow \mathbb{R}^{n}$$被称为一个 $n$ 维随机变量，如果对任意 $B \in \mathcal{B}$，我们有 $$\boldsymbol{X}^{-1}(B) \in \mathcal{U}.$$我们也称 $\boldsymbol{X}$ 是 $\mathcal{U}$-可测的。
> 这说的其实是 $\boldsymbol{X}$ 是 $\mathcal{U}$ 到 $\mathcal{B}$ 的一个**可测映射**？

我们注意这里的 $X^{-1} \in \mathcal{U}$，这样定义是比像要更好的，需要记住的是，$\mathcal{U}$ 和 $\mathcal{B}$ 都是 $\sigma$-代数，所以必须满足定义 2.2 中所述的若干条件。考虑 $B_{1}, B_{2} \in \mathcal{B}$，我们理应有
$$
\begin{align}
\boldsymbol{X}^{-1}(B_{1} \cup B_{2}), \boldsymbol{X}^{-1}(B_{1} \cap B_{2}), \boldsymbol{X}^{-1}(B_{1}^{c})\in \mathcal{U}.
\end{align}
$$
事实上，我们有集合的交并补可以穿透原像算子，也即
$$
f^{-1}(A \,\square\, B) = f^{-1}(A) \,\square\, f^{-1}(B),\quad f^{-1}(A^{c}) = [f^{-1}(A)]^{c}
$$
其中 $\square \in \{ \cup, \cap \}$。假如我们将原像算子换成看似更加自然的求像集操作，也即对任意 $U \in \mathcal{U}$，有 $f(U) \in \mathcal{B}$。由类似地逻辑，我们需要验证对任意的 $U, V \in \mathcal{U}$，都有
$$
\boldsymbol{X}(U \,\square\, V) \in \mathcal{B}, \boldsymbol{X}(U^{c}) \in \mathcal{B}.
$$
而我们可以适当地构造反例让上面的条件不再成立。

> **术语约定 2.10**
> 1. 我们一般写 $\boldsymbol{X}$，而不写 $\boldsymbol{X}(\omega)$
> 2. 我们一般写 $P(\boldsymbol{X} \in B)$，而不写 $P(\boldsymbol{X}^{-1}(B))$

> **示例 2.11 （示性函数，简单函数）** 考虑集合 $A \in \mathcal{U}$，它的示性函数定义为 $$\chi_{A}(\omega) := \begin{cases} 1, & \omega \in A\\ 0, & \omega \notin A, \end{cases}$$这是一个随机变量。更一般地，考虑 $A_{1}, \dots, A_{m} \in \mathcal{U}$，并有 $\displaystyle \Omega = \bigcup_{i=1}^{m} A_{i}$，$a_{i} \in \mathbb{R}$，于是 $$X = \sum\limits_{i=1}^{m} a_{i}\chi_{A_{i}}$$是一个随机变量，我们称其为简单函数（simple function）
> 简单函数是可覆盖概率空间 $\Omega$ 的一族 $\mathcal{U}$ 中元素之示性函数的线性组合。

> **引理 2.12 （随机变量生成的 $\sigma$-代数）** 设 $\boldsymbol{X}: \Omega \rightarrow \mathbb{R}^{n}$ 是一个随机变量，则 $$\mathcal{U}(\boldsymbol{X}) := \{ \boldsymbol{X}^{-1}(B): B \in \mathcal{B} \}$$是一个 $\sigma$-代数，被称为随机变量 $\boldsymbol{X}$ 生成的 $\sigma$-代数。
> 显然，$\mathcal{U}(\boldsymbol{X})$ 要比原来的 $\mathcal{U}$ 小。

这是使得 $\boldsymbol{X}$ 可测的最小的 $\mathcal{U}$ 的子$\sigma$-代数。

**证明.** 不难发现，这个引理需要用到前面提到的集合运算能够穿透映射原像的一个结果。
首先，空集和全集 $\Omega$肯定是 $\mathcal{U}(\boldsymbol{X})$的集合。对于前者，考虑 $\boldsymbol{X}^{-1}(\varnothing) = \{ \omega \in \Omega: f(\omega) \in \varnothing \}$，因为 $f(\omega) \in \varnothing$ 永远不可能为真，因此只有 $X^{-1}(\varnothing) = \varnothing$。另一方面，考虑 $\boldsymbol{X}^{-1}(\mathbb{R}^{n})$（由拓扑学知识，我们知道 $\mathbb{R}^{n}$ 是它自己的的开子集），我们得到 $\boldsymbol{X}^{-1}(\mathbb{R}^{n}) = \Omega$。
然后我们证明第二条，也即对任意 $B \in \mathcal{B}$，都有 $\boldsymbol{X}^{-1}(B)^{c} \in \mathcal{U}(\boldsymbol{X})$。事实上，我们来证明 $\boldsymbol{X}^{-1}(B)^{c}$ 事实上就是 $\boldsymbol{X}^{-1}(B^{c})$：
$$
\begin{align}
\boldsymbol{X}^{-1}(B)^{c} &= \{ \omega \in \Omega: f(\omega) \in B \}^{c}\\
&= \{ \omega \in \Omega: f(\omega) \in B^{c} \} = \boldsymbol{X}^{-1}(B^{c})
\end{align}
$$
用类似的方法，我们也可以证明该集族对可数并和可数交都封闭。于是 $\mathcal{U}(\boldsymbol{X})$ 是一个 $\sigma$-代数，且它不能再小了：从里面拿出一个元素都会使得 $\boldsymbol{X}$ 在拿掉元素后的集族下不可测。    Q.E.D.

> <font color="red"><b>注释 2.13</b></font> 很重要的一点是，我们从概率论的角度可以说 “$\mathcal{U}(\boldsymbol{X})$ 包含了 $\boldsymbol{X}$ 的相关信息”。假如另一个随机变量 $\boldsymbol{Y}$ 可以写成 $\boldsymbol{Y} = \Phi(\boldsymbol{X})$，其中 $\Phi$ 是某个 “合理的” 函数，那 $\boldsymbol{Y}$ 也是 $\mathcal{U}(\boldsymbol{X})$-可测的。另一方面，假如存在一个 $\mathcal{U}(\boldsymbol{X})$-可测的随机变量 $\boldsymbol{Y}$，那么一定存在一个函数 $\Phi$，使得 $\boldsymbol{Y} = \Phi(\boldsymbol{X})$。
> 事实上，这样的 $\Phi$ 称为 **可测映射**，这是了可测空间范畴的态射。

### 2.1.4 随机过程
本节中我们介绍取决于时间的随机变量。

> **定义 2.14 （随机过程，采样路径）** 一个随机变量的集合 $\{ \boldsymbol{X}(t): t \geqslant 0 \}$ 被称为一个随机过程。对每个 $\omega \in \Omega$，映射 $t \mapsto \boldsymbol{X}(t, \omega)$ 称为一个采样路径。

> <font color="red"><b>术语约定 2.15</b></font>
> 有的地方也使用 $\boldsymbol{X}_{t}$ 而不是 $\boldsymbol{X}(t)$ 指称一个关于变量 $t$ 的随机过程 

## 2.2 期望和方差
### 2.2.1 关于测度的积分

> **定义 2.16 （随机变量 $X$ 关于概率测度 $P$ 的积分）**
> 设 $(\Omega, \mathcal{U}, P)$ 是概率空间，$\displaystyle X = \sum\limits_{i=1}^{k} a_i\chi_{A_{i}}$ 是一个实值简单随机变量，定义 $X$ 的积分为 $$\int_{\Omega} X \, \mathrm dP := \sum\limits_{i=1}^{k} a_{i}P(A_{i}) $$如果 $X$ 是非负的随机变量，我们用简单随机变量积分的上确界定义它的积分 $$\int_{\Omega} X \, \mathrm dP := \sup_{Y \leqslant X, Y\text{ simple}} \int_{\Omega} Y \, \mathrm dP.$$对于一般的实值随机变量 $X:\Omega \rightarrow \mathbb{R}$，定义其积分值为正部的积分减去负部的积分（若二者至少有一个有限）：$$\int_{\Omega} X \, \mathrm dP := \int_{\Omega} X^{+} \, \mathrm dP - \int_{\Omega} X^{-} \, \mathrm dP,$$其中 $X^{+} := \max\{ X, 0 \}$，$X^{-} := \max\{ -X, 0 \}$；容易验证 $X = X^{+} - X^-$。

> **注解 2.16**
> 1. 事实上，这里的积分就是随机变量 $X$ 在概率测度 $P$ 下的 **Lebesgue 积分**，不难发现我们可以将其理解为对 “值域进行分割”。
> 2. 我们假设后面出现的所有积分都存在且有限

> **术语约定 2.17**
> 对于 $n$ 维随机变量 $\boldsymbol{X}: \Omega \rightarrow \mathbb{R}^{n}$，其中 $\boldsymbol{X} = (X^{1}, \dots, X^{n})$，我们将其积分写为 $$\int_{\Omega} \boldsymbol{X} \, \mathrm dP = \left[ \int_{\Omega} X^{1} \, \mathrm d{P}, \cdots, \int_{\Omega} X^{n} \, \mathrm d{P}  \right]$$

> **定义 2.18 （期望，方差）**
> 假设 $\boldsymbol{X}: \Omega \rightarrow \mathbb{R}^{n}$ 是一个向量值的随机变量，称 $$E(\boldsymbol{X}):= \int_{\Omega} \boldsymbol{X} \, \mathrm dP $$为它（在概率测度 $P$ 下）的期望（或均值），称 $$V(\boldsymbol{X}) := \int_{\Omega} \|\boldsymbol{X} - E(\boldsymbol{X})\|_{2}^{2}  \, \mathrm dP $$为它的方差。不难验证 $V(\boldsymbol{X}) = E(\|\boldsymbol{X}\|^{2}_{2}) - \|E(\boldsymbol{X})\|^{2}_{2}$.

### 2.2.2 分布函数

本节中我们默认概率空间是 $(\Omega, \mathcal{U}, P)$，并有一个随机变量 $\boldsymbol{X} : \Omega \rightarrow \mathbb{R}^{n}$。

> **术语约定 2.19**
> 取向量 $x = (x_{1}, \dots, x_{n}), y = (y_{1}, \dots, y_{n}) \in \mathbb{R}^{n}$，如果对每一个 $i = 1, \dots, n$，都有 $x_{i} \leqslant y_{i}$，我们就说 $x  \leqslant y$。
> 事实上这在 $\mathbb{R}^{n}$ 上构造了一个偏序。

> **定义 2.20 （分布函数）**
> 随机变量 $\boldsymbol{X}$ 的分布函数 $F_{\boldsymbol{X}}: \mathbb{R}^{n} \rightarrow [0, 1]$ 定义如下 $$F_{\boldsymbol{X}}(x) \coloneqq P(\boldsymbol{X} \leqslant x), \quad \text{for all }x \in \mathbb{R}^{n}$$更一般地，对于多个随机变量 $\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}: \Omega \rightarrow \mathbb{R}^{n}$，它们的**联合分布函数** $F_{\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}}: (\mathbb{R}^{n})^{m} \rightarrow [0, 1]$ 定义为 $$F_{\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}}(x_{1}, \dots, x_{m}) \coloneqq P(\boldsymbol{X}_{1} \leqslant x_{1}, \dots, \boldsymbol{X}_{m} \leqslant x_{m})$$其中 $x_{k} \in \mathbb{R}^{k}, k = 1, \dots, m$

> **定义 2.21（密度函数）**
> $\boldsymbol{X}: \Omega \rightarrow \mathbb{R}^{n}$ 是随机变量，$F = F_{\boldsymbol{X}}$ 是它的分布函数。假如存在一个非负可积的函数 $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ 使得 $$F(x) = F(x_{1}, \dots, x_{n}) = \int_{-\infty}^{x_{1}} \cdots  \int_{-\infty}^{x_{n}} f(y_{1}, \dots, y_{n}) \, \mathrm d{y_{n}}\cdots   \, \mathrm dy_{1} $$则我们称 $f$ 是 $\boldsymbol{X}$ 的密度函数。进而可以得到对任意的 $B \in \mathcal{B}$，都有 $$P(\boldsymbol{X} \in B) = \int_{B} f(x) \, \mathrm d x.$$

> **引理 2.22**
> 如果随机变量 $\boldsymbol{X}$ 的分布函数 $F_{\boldsymbol{x}}$ 存在对应的密度函数 $f$，并考虑 $g: \mathbb{R}^{n} \rightarrow \mathbb{R}$ ，且 $g(\boldsymbol{X})$ 是可积的（这里 $g$ 可积和 $g(\boldsymbol{X})$ 可积是一个意思吗？）则有 $$E(g(\boldsymbol{X})) = \int_{\mathbb{R}^{n}} g(x)f(x) \, \mathrm dx $$特别地，有 $$E(\boldsymbol{X}) = \int_{\mathbb{R}^{n}} xf(x) \, \mathrm dx,\quad V(\boldsymbol{X}) = \int_{\mathbb{R}^{n}} |x - m|^{2}f(x) \, \mathrm dx  $$其中 $m \coloneqq E(\boldsymbol{X})$。

> **注释 2.23**
> 这样一来我们就能通过在 $\mathbb{R}^{n}$ 上求积分的方式得到 $E(\boldsymbol{X})$ 和 $V(\boldsymbol{X})$。但需要注意概率空间 $(\boldsymbol{\Omega}, \mathcal{U}, P)$ 是 “难以看见的”，而我们所看见的是经过 $\boldsymbol{X}$ 映射后在 $\mathbb{R}^{n}$ 中的东西。
> 事实上，所有概率论中感兴趣的量都可以在 $\mathbb{R}^{n}$ 中通过密度函数 $f$ 求得

**证明.** 我们从简单的情况开始。首先假设 $g$ 是 $\mathbb{R}^{n}$ 上的一个简单函数：
$$
g = \sum\limits_{i=1}^{m} b_{i} \chi_{B_{i}}, \quad B_{i} \in \mathcal{B}
$$
这样就有
$$
E(g(\boldsymbol{X})) = \sum\limits_{i=1}^{m} b_{i} \int_{_{\Omega}} \chi_{B_{i}}(\boldsymbol{X}) \, \mathrm dP = \sum\limits_{i=1}^{m} b_{i}P(X \in B_{i}) 
$$
另一方面
$$
\int_{\mathbb{R}^{n}} g(x)f(x) \, \mathrm dx = \sum\limits_{i=1}^{m} b_{i} \int_{\mathbb{R}^{n}} f(x)\chi_{B_{i}} \, \mathrm dx = \sum\limits_{i=1}^{m} b_{i} \int_{B_{i}} f(x) \, \mathrm dx = \sum\limits_{i=1}^{m} b_{i}P(x \in B_{i}).   
$$
这样我们就得到当 $g$ 是简单函数时引理成立。由于一般的函数可由简单函数逼近，对于一般的函数上面的引理也成立。 Q.E.D.


## 2.3 独立性
### 2.3.1 条件概率

考虑概率空间 $(\Omega, \mathcal{U}, P)$，其中 $A, B \in \mathcal{U}$ 是两个事件，且 $P(B) > 0$。我们希望找到下面的 **条件概率** 的合理定义。
$$
P(A|B) = \text{给定 }B\text{ 条件下 }A\text{ 的概率}
$$

![[Pasted image 20250714131124.png]]
我们可以画出这张图，$\Omega$ 里面有一个点 $w \in B$，我们想知道它属于 $A$ 的概率。我们可以将 $B$ 视为一个新的概率空间：$\tilde{\Omega} \coloneqq B$，$\tilde{\mathcal{U}} \coloneqq \{ C \cup B: C \in \mathcal{U} \}$，$\displaystyle \tilde{P} \coloneqq \frac{P}{P(B)}$。我们立刻有 $\tilde{P}(\tilde{\Omega}) = 1$，$\displaystyle \tilde{P}(A \cap B) = \frac{P(A \cap B)}{P(B)}$

> **定义 2.24（条件概率）**
> 事件 $B$ 满足 $P(B) > 0$，定义事件 $B$ 发生下事件 $A$ 的条件概率为 $$P(A|B) \coloneqq \frac{P(A \cap B)}{P(B)}.$$


### 2.3.2 独立事件

明确了条件概率后我们就要看看事件之间相互独立是指什么了。这应该对应着 $P(A|B) = P(A)$，这可以理解为知道事件 $B$ 的发生与否对事件 $A$ 的发生与否没有贡献。因此假设 $P(B) > 0$，我们就有 $P(A \cap B) = P(A)P(B)$，下面我们依照这个事实对独立性做一个定义

> **定义 2.25（两事件的独立性）**
> 两个事件 $A$ 和 $B$ 独立，是指 $$P(A \cap B) = P(A)P(B)$$

假如两个事件 $A$ 和 $B$ 相互独立，那么 $A^{c}$ 和 $B$ ，$A$ 和 $B^{c}$，$A^{c}$ 和 $B^{c}$ 也是相互独立的。进一步地我们可以将定义扩展到多个事件

> **定义 2.26（相互独立的事件族）**
> 一族事件 $A_{1}, \dots, A_{n}, \dots$ 是相互独立是指对任取的 $1 \leqslant k_{1} < k_{2} <\cdots<k_{m}$，有 $$P(A_{k_{1}} \cap A_{k_{2}} \cap \cdots  \cap A_{k_{m}}) = P(A_{k_{1}})P(A_{k_{2}}) \cdots  P(A_{k_{m}}).$$

我们还可以定义 $\sigma$-代数 之间的独立关系

> **定义 2.27（相互独立的 $\sigma$-代数族）**
> 一组子 $\sigma$-代数 $\mathcal{U}_{i} \subset \mathcal{U}$，其中 $i = 1, \dots$ 是独立的，是指对任取的 $1 \leqslant k_{1} < k_{2} <\cdots<k_{m}$ 和其中的任意事件 $A_{k_{i}} \in \mathcal{U}_{k}$，有 $$P(A_{k_{1}} \cap A_{k_{2}} \cap \cdots  \cap A_{k_{m}}) = P(A_{k_{1}})P(A_{k_{2}}) \cdots  P(A_{k_{m}}).$$

### 2.3.3 独立随机变量

接下来我们考虑随机变量的独立性。

> **定义 2.28（相互独立的随机变量族）**
> 考虑一族随机变量 $\boldsymbol{X}_{i}: \Omega \rightarrow \mathbb{R}^{n}, i = 1, \dots$，假如对任意整数 $k \geqslant 2$，任取的 Borel 集 $B_{1}, \dots, B_{k} \subset \mathbb{R}^{n}$，有 $$P(\boldsymbol{X}_{1} \in B_{1}, \boldsymbol{X}_{2} \in B_{2}, \dots, \boldsymbol{X}_{k} \in B_{k}) = P(\boldsymbol{X}_{1} \in B_{1})P(\boldsymbol{X}_{2} \in B_{2})\cdots P(\boldsymbol{X}_{k} \in B_{k}),$$就称 $\boldsymbol{X}_{1}, \dots$ 是相互独立的。这相当于说它们生成的 $\sigma$-代数 $\{ \mathcal{U}_{i} \}_{i=1}^{\infty}$ 是相互独立的。


> **示例 2.29（Rademacher 函数）**
> 取 $\Omega = [0, 1)$，$\mathcal{U}$ 为 $[0, 1)$ 上的 Borel 子集，$P$ 为 Lebesgue 测度。对 $n = 1, 2, \dots$，定义 $$X_{n}(\omega) = \begin{cases}1 &\displaystyle  \text{if } \frac{k}{2^{n}} \leqslant \omega \leqslant  \frac{k+1}{2^{n}}, k \in 2\mathbb{Z} \\-1  & \displaystyle \text{if }  \frac{k}{2^{n}} \leqslant \omega \leqslant  \frac{k+1}{2^{n}}, k \in 2\mathbb{Z}+1\end{cases}, \quad 0 \leqslant \omega \leqslant 1.$$这样的函数叫做 Rademacher 函数，可以验证它们是相互独立的。仔细观察可以验证，该函数与小数的二进制表示有关。如果二进制小数 $x$ 的第 $k$ 位小数是 $0$，则 $X_{k}(x) = 1$，反之则为零。因此给定一个 $x \in [0, 1)$，序列 $X_{1}(x), X_{2}(x), \dots$ 与 $x$ 的二进制表示一一对应。现在我们考虑 $e_{1}, \dots, e_{k} \in \{ -1, +1 \}$，并验证 $$P(\boldsymbol{X}_{1} = e_{1}, \dots, \boldsymbol{X}_{k} = e_{k}) = P(\boldsymbol{X}_{1} = e_{1})\cdots P(\boldsymbol{X}_{k} = e_{k}).$$上面式子的左边其实是确定了二进制小数前 $k$ 位的值，因此取值的浮动只能在 $\displaystyle \frac{1}{2^{k}}$ 的范围内，我们考虑 Lebesgue 测度，因此左边就等于 $\displaystyle \frac{1}{2}$。接下来考虑右边。观察函数可以得到，不论下标和 $e_{i}$ 取何值，都有 $\displaystyle P(\boldsymbol{X}_{i} = e_{i}) \equiv \frac{1}{2}$，而右边刚好有 $k$ 项。由于 $k$ 是任取的，我们就证明了 $\boldsymbol{X}_{1}, \dots$ 是相互独立的。
> <font color="red">这其实也是符合直觉的，因为二进制小数部分的每一位是可以自由选择的，不存在选择了某一位之后另外一位的取值就受到影响。</font>


> **定理 2.30（相互独立的随机变量族划分后构造的新随机变量相互独立）** 
> 考虑相互独立的随机变量 $\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m+n}$，并考虑函数 $f: (\mathbb{R}^{k})^{n} \rightarrow \mathbb{R}$，$g: (\mathbb{R}^{k})^{m} \rightarrow \mathbb{R}$，则 $$\boldsymbol{Y} \coloneqq f(\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{n}), \quad \boldsymbol{Z} \coloneqq g(\boldsymbol{X}_{n+1}, \dots, \boldsymbol{X}_{n+m})$$这两个随机变量是独立的。


> **定理 2.31（相互独立随机变量的联合分布函数/分布密度函数满足乘积性质）**
> 一族 $n$ 维实值随机变量 $\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}: \Omega \rightarrow \mathbb{R}^{n}$ 相互独立，当且仅当 $$F_{\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}}(x_{1}, \dots, x_{m}) = F_{\boldsymbol{X}_{1}}(x_{1}) \cdots  F_{\boldsymbol{X}_{m}}(x_{m})$$其中 $x_{k} \in \mathbb{R}^{n}, k = 1, \dots, m$。
> 如果它们都存在对应的密度函数，这相当于 $$f_{\boldsymbol{X}_{1}, \dots, X_{n}}(x_{1}, \dots, x_{m}) = f_{\boldsymbol{X}_{1}}(x_{1}) \cdots  f_{\boldsymbol{X}_{m}}(x_{m})$$

**证明.**
一方面，假如 $\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}$ 相互独立，根据联合分布函数的定义，就有
$$
\begin{align}
F_{\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}}(x_{1}, \dots, x_{m}) &= P(\boldsymbol{X}_{1} \leqslant x_{1}, \dots, \boldsymbol{X}_{m} \leqslant x_{m}) \\
 & = P(\boldsymbol{X}_{1} \leqslant x_{1}) \cdots P(\boldsymbol{X}_{m} \leqslant x_{m})  & \text{随机变量的独立性} \\
 & = F_{\boldsymbol{X}_{1}}(x_{1}) \cdots  F_{\boldsymbol{X}_{m}}(x_{m}) & \text{分布函数的定义}
\end{align}
$$
另一方面，假如所有随机变量有对应的密度函数，对每个 $i=1, \dots, m$，选一个 $A_{i} \in \mathcal{U}(\boldsymbol{X}_{i})$ 所以存在某个 $B_{i} \in \mathcal{B}$，使得 $A_{i} = \boldsymbol{X}_{i}^{-1}(B_{i})$。因此
$$
\begin{align}
P(A_{1} \cap \cdots  \cap A_{m})  & = P(\boldsymbol{X}_{1} \in B_{1}, \dots, \boldsymbol{X}_{m} \in B_{m}) \\
 & = \int_{B_{1} \times \cdots  \times B_{m}} f_{\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}}(x_{1}, \dots, x_{m}) \, \mathrm d{x_{1}} \cdots \mathrm{d} x_{m} \\
 & = \left( \int_{B_{1}} f_{\boldsymbol{X}_{1}}(x_{1}) \, \mathrm d{x_{1}}  \right) \cdots  \left( \int_{B_{m}} f_{\boldsymbol{X}_{m}}(x_{m}) \, \mathrm d{x_{m}}  \right)  & \text{第二个条件} \\
 & = P(\boldsymbol{X}_{1} \in B_{1}) \cdots P(\boldsymbol{X}_{m} \in B_{m}) \\
 & = P(A_{1}) \cdots  P(A_{m})
\end{align}
$$
因此 $\mathcal{U}(\boldsymbol{X}_{1}), \dots, \mathcal{U}(\boldsymbol{X}_{m})$ 是相互独立的 $\sigma$-代数。


> **定理 2.32（相互独立且存在期望的随机变量的联合期望等于单独期望的乘积）**
> 一族实值随机变量 $X_{1}, \dots, X_{m}$ 相互独立，且 $E(|X_{i}|) < \infty, i = 1, \dots, m$，则有 $$\begin{align}E(|X_{1}\cdots X_{m}|) &< \infty \\E(X_{1}\cdots X_{m}) & =E(X_{1}) \cdots E(X_{m}).\end{align}$$

**证明.**
假设每个随机变量都有界，且存在对应的密度函数，于是
$$
\begin{align}
E(X_{1}, \dots, X_{m}) & = \int_{\mathbb{R}^{m}} x_{1}\cdots x_{m}f_{X_{1}, \dots, X_{m}}(x_{1}, \dots, x_{m}) \, \mathrm d{x_{1}\dots x_{m}} \\
 & =  \left( \int_{\mathbb{R}} x_{1}f_{X_{1}}(x_{1}) \, \mathrm d{x_{1}}  \right)\cdots \left( \int_{\mathbb{R}} x_{m}f_{X_{m}}(x_{m})  \, \mathrm d{x_{m}}  \right) & \text{密度函数性质、拆开积分区域} \\
 & = E(X_{1}) \cdots  E(X_{m})
\end{align}
$$

> **定理 2.33（相互独立方差存在的随机变量之和的方差等于方差之和）**
> 一族实值随机变量 $X_{1}, \dots, X_{m}$ 相互独立，且 $V(X_{i}) < \infty, i = 1, \dots, m$，则有 $$V(X_{1} + \cdots  + X_{m}) = V(X_{1}) + \cdots  + V(X_{m}).$$

**证明.**
我们用归纳法证明。当 $m=2$ 时，令 $m_{1} = E(X_{1}), m_{2} = E(X_{2})$，于是有 $E(X_{1} + X_{2}) = m_{1} + m_{2}$，以及
$$
\begin{align}
V(X_{1}+X_{2})  & = \int_{\Omega} (X_{1}+X_{2} - (m_{1}+m_{2}))^{2} \, \mathrm d{P}  \\
 & = \int_{\Omega} (X_{1} - m_{1})^{2} \, \mathrm d{P}  + \int_{\Omega} (X_{2} - m_{2})^{2}  \, \mathrm d{P} + 2\int_{\Omega} (X_{1}-m_{1})(X_{2}-m_{2}) \, \mathrm d{P}  \\
 & = V(X_{1}) + V(X_{2}) + 2E\big[ (X_{1} - m_{1})(X_{2} - m_{2}) \big] \\

 & = V(X_{1}) + V(X_{2}) + \underbrace{ 2E\big[X_{1} - m_{1}\big] E\big[X_{2} - m_{2}\big] }_{ 0 }  & \text{定理 2.32}\\
\end{align}
$$
然后我们就可以用 2.30 不断构造新的随机变量，完成我们的归纳法证明。

## 2.4 一些概率论中的工具和方法
### 2.4.1 Chebyshev 不等式

> **引理 2.34（Chebyshev）**
> $\boldsymbol{X}$ 是一个随机变量，$p$ 满足 $1 \leqslant p < \infty$，则对任意 $\lambda > 0$，有 $$P(|\boldsymbol{X}| \geqslant \lambda) \leqslant \frac{1}{\lambda^{p}} E(|\boldsymbol{X}|^{p})$$

**证明.**
$$
E(|\boldsymbol{X}|^{p}) = \int_{\Omega} |\boldsymbol{X}|^{p} \, \mathrm d{P} \geqslant \int_{|\boldsymbol{X}| \geqslant \lambda} |\boldsymbol{X}|^{p} \, \mathrm d{P} \geqslant \lambda^{p}P(|\boldsymbol{X}|\geqslant\lambda). 
$$

### 2.4.2 Borel–Cantelli 引理

> **定义 2.35（无限发生的事件）**
> 给定概率空间中的事件序列 $A_{1}, \dots, A_{n}, \dots$，其中发生无限次的事件的集合可以写为 $$\bigcap_{n=1}^{\infty} \bigcup_{m=n}^{\infty} A_{m} = \limsup_{ n \to \infty } A_{n}.$$记作 $A_{n}\;i.o.$

定义中里面的可数无穷并说的是确定一个 $n$，然后把所有指标大于 $n$ 的事件收集起来；前面一个无穷交相当于取了一个极限，所有只在事件序列中出现有限次的点 $\omega$ 都会被过滤掉，剩下的就是发生了无限次的事件。例如掷两枚硬币，第一枚是理想的硬币，一直掷下去就会得到 $THTTH H T\cdots$ 这样的序列，正面和翻面都会出现无限次，因此对应的发生无限次的事件的集合就是 $\{ T, H \}$；假如第二枚硬币是做了手脚的，扔了几次之后每次抛掷得到的结果总是正面，对应发生无限次的事件的集合就是 $\{ H \}$ 因为 $T$ 只发生了有限次，它就在上面去极限的过程中被过滤掉了。

> **引理 2.36（Bodel-Cantelli）**
> 如果一个事件序列 $A_{1}, \dots$ 满足 $\displaystyle \sum\limits_{n=1}^{\infty} P(A_{n}) < \infty$，则 $$P(A_{n}\;i.o.) = 0.$$

**证明.**
根据定义，对每个 $n$，有
$$
P(A_{n}\;i.o.) = P\left(\bigcap_{n=1}^{\infty} \bigcup_{m=n}^{\infty} A_{m} \right) \leqslant P\left( \bigcup_{m=n}^{\infty}A_{m}  \right) \leqslant \sum\limits_{m=n}^{\infty} P(A_{m}).
$$
因为级数 $\displaystyle \sum\limits_{n=1}^{\infty} P(A_{n}) < \infty$，令 $n \rightarrow \infty$，就能得到 $P(A_{n}\;i.o.) \rightarrow 0$。

考虑定义在某个概率空间的随机变量序列 $\{ X_{k} \}_{k=1}^{\infty}$ **依概率收敛** 到某一随机变量 $X$ 指的是对任意 $\epsilon > 0$，都有 $$\lim_{ k \to \infty } P(|X_{k} - X| > \epsilon) = 0.$$
事实上我们有下面的定理，揭示了上述例子和 Borel-Cantelli 引理之间的关系

> **定理 2.37（依概率收敛的随机变量子列几乎必然收敛到原极限）**
> 如果 $X_{k}$ 依概率收敛到 $X$（记为 $X_{k} \xrightarrow{P} X$）则存在一个子序列 $\{ X_{k_{j}} \}_{j = 1}^{\infty} \subset \{ X_{k} \}_{k = 1}^{\infty}$，使得 $$X_{k_{j}} \rightarrow X\quad a.s.$$

**证明.** 对每个正整数 $j$，我们找一个很大的 $k_{j}$，使其满足 
$$P\left( |X_{k_{j}} - X| > \frac{1}{j} \right) \leqslant \frac{1}{j^{2}},$$
同时保证 $k_{1} < \cdots < k_{j-1} < k_{j} < \cdots$ 因此显然 $k_{j} \rightarrow \infty$。令 $\displaystyle A_{j} := \left\{  |X_{k_{j}} - X| > \frac{1}{j} \right\}$，由于每个 $A_j$ 发生概率的上界构成的级数 $\displaystyle\sum \frac{1}{j^{2}}  < \infty$，因此可以使用 Borel-Cantelli 引理，就有 $P(A_{j}\quad \text{i.o.}) = 0$。这句话的意思是，几乎每个样本空间的样本点 $\omega$，$A_{j}$ 发生的次数是有限的（否则如果可能发生无限次，$A~ ~\text{i.o.}$ 的概率就不为零了，另一方面 $A_{j}$ 发生无限次的样本点集合的概率测度为零），因此我们能找到 $A_{j}$ 最后一次发生的指标，然后选取一个比它大的 $J(\omega)$，对所有 $j > J(\omega)$，有 $\displaystyle |X_{k_{j}}(\omega) - X(\omega)| \leqslant \frac{1}{j}$，这时候把 $j$ 推向无穷大，就有 $X_{k_{j}}(\omega) \rightarrow X(\omega)~ ~\text{a.s.}$。 

### 2.4.3 特征函数

> **定义 2.38 （随机变量的特征函数）**
> 假设 $\boldsymbol{X}$ 是一个 $n$ 维实值随机变量，定义其特征函数为 $$\phi_{\boldsymbol{X}}(\lambda) := E(e^{\text{i}\left\langle \boldsymbol{\lambda}, \boldsymbol{X} \right\rangle}), \quad \boldsymbol{\lambda} \in \mathbb{R}^{n}$$

> **示例 2.39 （Gaussian 随机变量的特征函数）**
> 考虑服从 $N(0, 1)$ 的随机变量，其特征函数是 
> $$\begin{align}\phi_{X}(\lambda) &= \int_{-\infty}^{\infty} e^{\text{i}\lambda x} \frac{1}{\sqrt{ 2\pi }} e^{-x^{2}/2} \, \mathrm d{x}\\&= \frac{e^{-\lambda^{2}/2}}{\sqrt{ 2\pi }}\int_{-\infty}^{\infty} e^{-(x - \text{i}\lambda)^{2}/2} \, \mathrm d{x}\end{align}$$
> 此时我们会想做变换 $t \leftarrow x + \text{i}\lambda$。而复分析给了我们依据。积分里面的函数在复平面上是解析的，根据 Cauchy 积分定理，我们将积分路径（实轴）往虚轴正方向移 $\lambda$ 得到的两条路径是同伦的（考虑两条路径在 Riemann 球面上的投影），因此积分值相同，而我们知道 $\displaystyle \int_{-\infty}^{\infty} e^{-x^{2}/2} \, \mathrm d{x} = \sqrt{ 2\pi }$，就得到 $\phi_{X}(\lambda) = e^{-\lambda^{2}/2}$。
> 一般的 Gaussian 随机变量也同理，最后得到的结果是 $\phi_{X}(\lambda) = e^{\text{i}m\lambda - \lambda^{2}\sigma^{2}/2}$

> **引理 2.40 （特征函数继承 Fourier 变换的性质）**
> （ 和的特征函数等于各自特征函数的积 ）如果 $\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{m}$ 是独立随机变量，对所有 $\boldsymbol{\lambda} \in \mathbb{R}^{n}$，有 $$\phi_{\boldsymbol{X}_{1} + \cdots + \boldsymbol{X}_{m}}(\boldsymbol{\lambda}) = \phi_{\boldsymbol{X}_{1}}(\boldsymbol{\lambda})\cdots \phi_{\boldsymbol{X}_{m}}(\boldsymbol{\lambda})$$
> （ 特征函数的 $k$ 阶导相当于将 $X^{k}$ 的特征函数在复平面内旋转 $k$ 次）如果 $X$ 是实值随机变量，则对任意非负整数 $k$，有 $$\phi^{(k)}(0) = \text{i}^{k} E(X^{k})$$
> （ 特征函数决定分布函数 ）$\boldsymbol{X}$ 和 $\boldsymbol{Y}$ 是两个随机变量，则 $$\phi_{\boldsymbol{X}}(\boldsymbol{\lambda}) = \phi_{\boldsymbol{Y}}(\boldsymbol{\lambda}) \implies F_{\boldsymbol{X}}(\boldsymbol{x}) = F_{\boldsymbol{Y}}(\boldsymbol{x})$$

**证明.** 
（1）
$$
\begin{align}
\phi_{\boldsymbol{X}_{1} + \cdots + \boldsymbol{X}_{m}}(\boldsymbol{\lambda}) &= E(e^{\text{i}\left\langle \boldsymbol{\lambda}, \boldsymbol{X}_{1} + \cdots + \boldsymbol{X}_{m} \right\rangle })\\
&= E\big( e^{\text{i}\left\langle \boldsymbol{\lambda}, \boldsymbol{X}_{1} \right\rangle } \cdots e^{\text{i}\left\langle \boldsymbol{\lambda}, \boldsymbol{X}_{m} \right\rangle } \big) & 内积线性性\\
&= E\big( e^{\text{i}\left\langle \boldsymbol{\lambda}, \boldsymbol{X}_{1} \right\rangle }\big) \cdots E\big(e^{\text{i}\left\langle \boldsymbol{\lambda}, \boldsymbol{X}_{m} \right\rangle } \big)&独立性\\
&=\phi_{\boldsymbol{X}_{1}}(\boldsymbol{\lambda})\cdots \phi_{\boldsymbol{X}_{m}}(\boldsymbol{\lambda})
\end{align}
$$
（2）（3）略

> **示例 2.41 （相互独立的 Gaussian 随机变量之和）**
> 两个随机变量 $X \sim N(m_{1}, \sigma_{1}^{2})$，$Y \sim N(m_{2}, \sigma_{2}^{2})$ ，则 $$X + Y \sim N(m_{1}+m_{2}, \sigma_{1}^{2} + \sigma^{2}_{2}),$$因为 $$\phi_{X+Y}(\lambda) = \phi_{X}(\lambda)\phi_{Y}(\lambda) = e^{\text{i}(m_{1}+m_{2})\lambda - \lambda^{2}(\sigma_{1}^{2}+\sigma_{2}^{2})/2}$$

## 2.5 大数定律和中心极限定理
### 2.5.1 独立同分布的随机变量

> **定义 2.42 （同分布）**
> 一族随机变量 $\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{n}, \dots$ 若对所有 $x$，满足 $$F_{\boldsymbol{X}_{1}}(x) = F_{\boldsymbol{X}_{2}}(x) = \cdots = F_{\boldsymbol{X}_{n}}(x) = \cdots$$则称它们是同分布的。

如果我们在同分布的基础上，嘉定这些随机变量是相互独立的，我们就可以将它们视为一系列结果可观测的独立重复试验，给定样本空间的点 $\omega \in \Omega$，我们可以得到随机变量序列在其上的取值序列。下面的内容揭示了这样的序列遵循的规律

### 2.5.2 强大数定律

> **定理 2.43 （强大数定律）**
> 考虑同一概率空间下一个独立同分布且可积的随机变量序列 $\boldsymbol{X}_{1}, \dots, \boldsymbol{X}_{n}, \dots$ ，记 $m = E(X_{i}), i=1, \dots$，则有 $$P\left( \lim_{ n \to \infty } \frac{\boldsymbol{X}_{1} + \cdots + \boldsymbol{X}_{n}}{n} = m \right) = 1. \quad 或者说 \quad \frac{1}{n}\sum\limits_{i=1}^{n} \boldsymbol{X}_{i} \rightarrow m\quad \text{a.s.}$$

**证明.**  不失一般性，我们假设这些随机变量都是实值的、四阶矩存在，且均值为零（假设不为零，我们就考虑 $\boldsymbol{X}_{i} - m$）。接下来考虑随机变量部分和的四阶矩：
$$
\begin{align}
E\left[ \left( \sum\limits_{i=1}^{n}{X}_{i} \right)^{4} \right] &= \sum\limits_{i,j,k,l} E(X_{i}X_{j}X_{k}X_{l}) 
\end{align}
$$
当求和指标中的 $i$ 和 $j,k,l$ 都不同时，根据独立性，有 $$E(X_{i}X_{j}X_{k}X_{l}) = \underbrace{ E(X_{i}) }_{ 0 } E(X_{j}X_{k}X_{l}) = 0$$由于 $i,j,k,l$ 这四个指标的地位是等同的，可以发现求和项里面只会剩下 $XXXX$ 或者 $XXYY$ 这样的组合了，因为其余的组合根据独立性都会变成零。因此四阶矩可以化简：
$$
\begin{align}
E\left[ \left( \sum\limits_{i=1}^{n}{X}_{i} \right)^{4} \right] &= \binom{4}{4} \sum\limits_{i=1}^{n} E(X_{i}^{4}) + \binom{4}{2} \sum\limits_{i,j=1, i \neq j} E(X_{i}^{2}X_{j}^{2})\\ 
&= \sum\limits_{i=1}^{n} E(X_{i}^{4}) + 3\sum\limits_{i,j=1, i \neq j} E(X_{i}^{2}X_{j}^{2})\\ 
&= nE(X_{1}^{4}) + 3(n^{2} - n)\big[ E(X_{1}^{2}) \big]^{2} \leqslant Cn^{2}& 独立同分布\\
\end{align}
$$
$C$ 是某个常数，这说明四阶矩的阶是 $O(n^{2})$。接下来我们来看我们得到的结果和我们要证明的目标的关系。回想前文提到的 Chebyshev 不等式，它说对于任意 $1 \leqslant p < \infty$ 和 $\lambda > 0$，有
$$
P(|\boldsymbol{X}| \geqslant \lambda) \leqslant \frac{1}{\lambda^{p}} E(|\boldsymbol{X}|^{p})
$$
令 $p = 4$，然后代入我们的证明目标：
$$
\begin{align}
P\left( \left|\frac{1}{n} \sum\limits_{i=1}^{n} X_{i} \right|\geqslant \varepsilon \right) &= P\left(\left|\sum\limits_{i=1}^{n} X_{i} \right|\geqslant n\varepsilon \right)\\
&\leqslant \frac{1}{(n\varepsilon)^{4}}E\left[ \left( \sum\limits_{i=1}^{n}{X}_{i} \right)^{4} \right] & \text{Chebyshev } 不等式\\
&\leqslant \frac{C}{\varepsilon^{4}n^{2}} & 被 ~\frac{1}{n^{2}}~ 控制
\end{align}
$$
注意上面的式子是和 $n$ 有关的，且因为组成的级数被 $\displaystyle \sum \frac{1}{n^{2}}$ 控制，因此有 $\displaystyle \sum\limits_{n} P(A_{n}) < \infty$。根据 Borel-Cantelli 引理，有
$$
P\left( \left|\frac{1}{n} \sum\limits_{i=1}^{n} X_{i} \right|\geqslant \varepsilon ~ ~\text{i.o.}\right) = 0
$$
取 $\displaystyle \varepsilon = \frac{1}{k}$，这就说明 $$\limsup_{ n \to \infty } \left| \frac{1}{n}\sum\limits_{i=1}^{n} X_{i} \right| \leqslant \varepsilon\quad \text{a.s.}$$也即除了样本空间中的一个零测集 $B_{k}$ 中的元素，其他元素都遵循上面的规则。令 $\displaystyle B = \bigcup_{k=1}^{\infty} B_{k}$，显然有 $P(B) = P(B_{k}) = 0$。把 $k$ 推向无穷大，对于所有 $\omega \in \Omega \setminus B$，都有 $$\lim_{ n \to \infty } \frac{1}{n}\sum\limits_{i=1}^{n} X_{i}(\omega) = 0.$$
也即 $\displaystyle \frac{1}{n}\sum\limits_{i=1}^{n} {X}_{i} \rightarrow m\quad \text{a.s.}$。

### 2.5.3 扰动和 Laplace-De Moivre 定理

我们已经有了大数定律，接下来我们研究这个极限中的扰动。

> **引理 2.44** 
> 假设 $X_{1}, \dots, X_{n}, \dots$ 独立同分布：$$\begin{cases}P(X_{i} = 1) = p\\P(X_{i} = 0) = q\end{cases}$$其中 $p,q \geqslant 0, p+q = 1$，则 $$\begin{align}E(X_{1} + \cdots + X_{n}) &= np,\\ V(X_{1} + \cdots + X_{n}) &= npq.\end{align}$$

> **定理 2.45 （Laplace-De Moivre）**
> $X_{1}, \dots, X_{n}$ 是独立同分布的实值随机变量，并有引理中的性质。定义 $S_{n} := X_{1} + \cdots + X_{n}$，则对所有的 $-\infty < a < b < +\infty$，有 $$\lim_{ n \to \infty } P\left( a \leqslant \frac{S_n - np}{\sqrt{ npq }} \leqslant b\right) = \frac{1}{\sqrt{ 2\pi }}\int_{a}^{b} {e^{-x^{2}/2}} \, \mathrm d{x}. $$

我们可以对该定理的结果做一些进一步发观察。首先有
$$
\frac{S_{n} - np}{\sqrt{ npq }} = \frac{S_{n} - E(S_{n})}{\sqrt{ V(S_{n}) }}
$$
可见 Laplace-De Moivre 定理指出标准化后的部分和 $S_{n}$ 当 $n \rightarrow \infty$ 时会趋于标准 Gaussian 分布 $N(0, 1)$。考虑 $\displaystyle p = q = \frac{1}{2}$ 的特殊情况，假设 $a > 0$，有 $$\lim_{ n \to \infty } P\left( -\frac{a\sqrt{ n }}{2}\leqslant S_{n} - \frac{n}{2} \leqslant \frac{a\sqrt{ n }}{2} \right) = \frac{1}{\sqrt{ 2\pi }} \int_{-a}^{a} e^{-x^{2}/2} \, \mathrm dx $$固定 $b>0$，并令 $\displaystyle a = \frac{2b}{\sqrt{ n }}$，则当 $n \rightarrow \infty$，有
$$
P\left( -b \leqslant S_{n} - \frac{n}{2} \leqslant b \right) \approx \frac{1}{\sqrt{ 2\pi }}\int_{-\frac{2b}{\sqrt{ n }}}^{\frac{2b}{\sqrt{ n }}} e^{-x^{2}/2} \, \mathrm dx \rightarrow 0. 
$$
根据强大数定律对几乎所有 $\omega$，有 $\displaystyle \frac{S_{n}(\omega)}{2} \rightarrow \frac{1}{2}$，然而根据上面的定理，$\displaystyle \left|S_{n}(\omega) - \frac{n}{2}\right|$ 必然会产生幅度超过任意有限值 $b$ 的震荡。

### 2.5.4 中心极限定理

> **定理 2.46 （中心极限定理）**
> 一列独立同分布的实值随机变量 $X_{1}, \dots, X_{n}, \dots$ 满足 $$E(X_{i}) = m, \quad V(X_{i}) = \sigma^{2} > 0,$$ 并记 $S_{n} := X_{1} + \cdots + X_{n}$，则对所有的 $-\infty < a < b < \infty$，有 $$\lim_{ n \to \infty } P\left( a \leqslant \frac{S_{n} - mn}{\sqrt{ n }\sigma} \leqslant b \right) = \frac{1}{2\pi} \int_{a}^{b} {e^{-x^{2}/2}} \, \mathrm d{x}.$$

读者可以看到中心极限定理的形式和刚刚见到的 Laplace-De Moivre 定理非常类似，可见其结论不仅适用于只有 $0,1$ 取值的随机变量，还可应用于更广泛的独立同分布、有限方差的随机变量的情形。

下面给出中心极限定理的大致证明框架。
首先假设 $m=0, \sigma=1$，然后对 $S_{n}$ 做标准化，因为它是 $n$ 个独立同分布的随机变量的和，所以除以 $\sqrt{ n }$。考虑标准化后随机变量的特征函数
$$
\phi_{\frac{S_{n}}{\sqrt{ n }}}(\lambda) = \phi_{\frac{X_{1}}{\sqrt{ n }}}(\lambda)\cdots\phi_{\frac{X_{n}}{\sqrt{ n }}}(\lambda) = \left(\phi_{X_{1}}\left(\frac{\lambda}{\sqrt{ n }}\right)\right)^{n}
$$
其中 $\lambda \in \mathbb{R}$。最后一个等号显示出的是特征函数的一个性质，系数从外面穿脱到里面，相当于在特征函数的定义里的实内积中把一边的系数移到了另一边。方便起见记 $\phi = \phi_{X_{1}}$，并考虑其在零处的 Taylor 展开
$$
\phi(\mu) = \phi(0) + \phi'(0)\mu + \frac{1}{2}\phi''(0)m\mu^{2} + o(\mu^{2}), \quad \mu \rightarrow 0
$$
根据特征函数性质，有 $\phi(0) = 1$，$\phi'(0) = \text{i}E(X_{1}) = 0$，$\phi''(0) = -E(X_{1}^{2}) = -1$。此时我们可以用上面的结果研究 $\displaystyle \phi_{X_{1}}\left(\frac{\lambda}{\sqrt{ n }}\right)$，设 $\displaystyle \mu = \frac{\lambda}{\sqrt{ n }}$，就有
$$
\phi_{X_{1}}\left( \frac{\lambda}{\sqrt{ n }} \right) = 1 - \frac{\lambda^{2}}{2n} + o\left( \frac{\lambda^{2}}{n} \right)
$$
进而对任意 $\lambda$，当 $n \rightarrow \infty$ 时，有
$$
\phi_{S_{n}}\left( \frac{\lambda}{\sqrt{ n }} \right) = \left[ 1 - \frac{\lambda^{2}}{2n} + o\left( \frac{\lambda^{2}}{n} \right) \right]^{n} = e^{-\lambda^{2}/2}
$$
而 $e^{-\lambda^{2}/2}$ 实际上就是标准 Gaussian 分布的特征函数。

## 2.6 条件期望
### 2.6.1 动机

我们之前定义了条件分布 $P(A|B)$，现在我们思考应该如何定义条件期望 $E(X|B)$ —— 给定事件 $B$ 下随机变量 $X$ 的期望？回忆我们在定义条件概率的时候构造了一个新的概率空间，其中概率测度是 $\displaystyle \tilde{P} = \frac{P}{P(B)}$，如果 $P(B) > 0$，那么给定事件 $B$ 下的条件期望就可以写成
$$
E(X|B) = \frac{1}{P(B)} \int_{B} X \, \mathrm dP. 
$$
这还没有结束。如果考虑给定随机变量 $Y$ 下随机变量 $X$ 的条件期望 $E(X|Y)$ 应该如何定义？或者说，对于样本空间 $\Omega$ 中的一个样本点 $\omega$，我们知道在在一个随机变量下的取值 $Y(\omega)$，我们如何对 $X(\omega)$ 作出最优的估计？

事实上这涉及到很多细节，并对概率论理论而言十分重要，下面我们将逐步介绍。

### 2.6.2 条件期望的第一种构造

> **示例 2.47**
> 考虑概率空间 $(\Omega, \mathcal{U}, P)$ 和定义在其上的随机变量 $Y$。定义 $\displaystyle Y = \sum\limits_{i=1}^{m} a_{i}\chi_{A_{i}}$，其中 $a_{i}$ 是两两不同的实数，$\{A_{i}\}$ 是 $\Omega$ 的一个划分。现在令 $X$ 是 $\Omega$ 上的另一个随机变量，已知 $Y$ 的值，我们需要估计 $X$ 在该条件下的期望，这似乎非常自然 —— 我们能找到对应的样本点属于哪一个事件，就在该事件发生的条件下计算 $X$ 的期望：$$E(X|Y) := \begin{cases}\displaystyle \frac{1}{P(A_{1})}\int_{A_{1}} X \, \mathrm dP, &\omega \in A_{1} \\\displaystyle \frac{1}{P(A_{2})}\int_{A_{2}} X \, \mathrm dP, &\omega \in A_{2} \\[0.5em]\quad\quad\quad\vdots\\\displaystyle \frac{1}{P(A_{m})}\int_{A_{m}} X \, \mathrm dP, &\omega \in A_{m} \\\end{cases}$$需要注意的是下面三点
> * $E(X|Y)$ 是一个随机变量，<font color="red"><big>不是一个常数</big></font>
> * $E(X|Y)$ 是 $\mathcal{U}$-可测的
> * 对所有 $A \in \mathcal{U}(Y)$，有 $\displaystyle \int_{A} X \, \mathrm dP = \int_{A} E(X|Y) \, \mathrm dP$

这就引出了下面的定义

> **定义 2.48 （随机变量对随机变量的条件期望）**
> $X$ 和 $Y$ 是定义在同一概率空间 $\Omega$ 下的随机变量，给定 $Y$ 下 $X$ 的条件期望是任何满足下面条件的 $\mathcal{U}$-可测随机变量 $Z$：$$\int_{A} X \, \mathrm dP = \int_{A} Z \, \mathrm dP, \quad\forall A \in \mathcal{U}(Y).$$

> **注解 2.49**
> 上面定义中的满足条件的 $Z$ 只可能在零测集中的取值有不同 —— 它们在几乎必然的意义下是唯一的。因此我们接下来将 $Z$ 写为 $E(X|Y)$

读者会发现，上述问题中出镜最多的不是随机变量 $Y$，而是它生成的 $\sigma$-代数。我们其实可以将其写为 $E(X|\mathcal{U}(Y))$。下面的定义将其做了扩展：我们可以写出任意概率空间的子 $\sigma$-代数的条件期望

> **定义 2.50 （随机变量对子 $\sigma$-代数的条件期望）**
> $(\Omega, \mathcal{U}, P)$ 是一个概率空间，$\mathcal{V} \subset \mathcal{U}$ 是一个子 $\sigma$-代数。如果 $X: \Omega \rightarrow \mathbb{R}^{n}$ 是一个可积随机变量，条件期望 $E(X|\mathcal{V})$ 是满足下面条件的 $\Omega$ 上的任意随机变量：
> * $E(X|\mathcal{V})$ 是 $\mathcal{V}$-可测的
> * 对所有 $A\in \mathcal{V}$，有 $\displaystyle \int_{A} X \, \mathrm dP = \int_{A} E(X|\mathcal{V}) \, \mathrm dP$

> **注解 2.50**
> 可以这么理解：$E(X|\mathcal{V})$ 给出了在拥有了 $\mathcal{V}$ 中信息的条件下随机变量 $X$ 的估计。
> 定义里面第一条说的是 $E(X|\mathcal{V})$ 可以由 $\mathcal{V}$ 中的信息构造
> 第二条说的是 $E(X|\mathcal{V})$ 必须和 $X$ 相容 —— 至少对 $\mathcal{V}$ 中的事件来说积分要一样
> 条件期望有下面几个简单性质
> 1. $E(X|Y) = E(X|\mathcal{U}(Y))$
> 2. $E(E(X|\mathcal{V})) = E(X)$
> 3. $E(X) = E(X|\mathcal{W})$，其中 $\mathcal{W} = \{ \varnothing, \Omega \}$ 是平凡 $\sigma$-代数

> **定理 2.51**
> $X$ 是可积随机变量，对任意子 $\sigma$-代数 $\mathcal{V} \subset \mathcal{U}$，条件期望 $E(X|\mathcal{V})$ 总是存在，并在允许概率为零的 $\mathcal{V}$-可测集取值不同的意义下都相等。

### 2.6.3 条件期望的第二种构造

我们首先看看一个看起来不怎么相关的问题 —— 最小二乘问题。

考虑 Euclid 空间 $(\mathbb{R}^{n}, |\cdot|)$ 和它的一个子空间 $V$。最小二乘问题说的是给定一个 $\mathbb{R}^{n}$ 中的元素 $x$，找一个 $V$ 中的元素 $z$ 使得它离 $x$ 之间的距离最近：$$
|z - x|^{2} = \min_{y \in V} |y - x|^{2}
$$我们称这样的 $z$ （若存在）是 $x$ 在 $V$ 上的 **投影**，记为 $\text{proj}_{V}(x)$。我们可以引入一个向量 $w \in V$ 来帮助我们找到这个问题的解。对于标量 $\tau$，定义
$$
i(\tau) := |z + \tau w - x|^{2}
$$
根据上文中 $z$ 的最优性，函数 $i(\tau)$ 必然在 $\tau = 0$ 处取到最小值，因为 $i(\tau)$ 是凸函数，必然有 $i'(0) = 0$，对任意的 $w$ 都有
$$
\big\langle z - x, w \big\rangle = 0 \iff \left\langle x, w \right\rangle  = \left\langle z, w \right\rangle 
$$
这说明 $z - x \perp V$ 或者说 $z - x \in V^{\perp}$。

把视角拉回条件期望。考虑 $L^{2}(\Omega) = L^{2}(\Omega, \mathcal{U})$，它是满足下面条件的 $\mathcal{U}$-可测的实值随机变量全体构成的线性空间，满足平方可积的条件：
$$
\|Y\|_{L^{2}(\Omega)} := \left[ \int_{\Omega} Y^{2} \, \mathrm dP  \right]^{1/2} < \infty,
$$
其中 $\|\cdot\|$ 是 $L^{2}(\Omega)$ 上的 **范数**。对于 $X, Y \in L^{2}(\Omega)$，定义它们的 **内积** 为
$$
\left\langle X, Y \right\rangle_{L^{2}(\Omega)} := \int_{\Omega} XY \, \mathrm dP = E(XY).
$$
现在考虑 $\mathcal{U}$ 的一个子 $\sigma$-代数 $\mathcal{V}$，和对应的 $V = L^{2}(\Omega, \mathcal{V})$ —— 所有平方可积且 $\mathcal{V}$-可测的随机变量构成的线性空间 —— 它是 $L^{2}(\Omega)$ 的一个闭子空间。仿照最小二乘的做法，我们也可以定义 $X \in L^{2}(\Omega)$ 到 $V$ 上的投影 $Z = \text{proj}_{V}(X)$。继续仿照上面引入一个 $V$ 中的向量 $W$，我们也可以得出相同的结论：
$$
\left\langle X, W \right\rangle_{L^{2}(\Omega)}  = \left\langle Z, W \right\rangle_{L^{2}(\Omega)} ,\quad \forall \,W \in V.
$$
我们取 $\mathcal{V}$ 中的任意事件 $A$，然后令 $W = \chi_{A}$，就有
$$
\int_{A} X \, \mathrm dP = \int_{A} Z \, \mathrm dP, \quad \forall\, A \in \mathcal{V}  .
$$
由于 $Z \in V$，$Z$ 是 $\mathcal{V}$-可测的，它其实就是条件期望 $E(X|\mathcal{V})$，我们至此就发现了它与投影之间的奇妙联系：
$$
E(X|\mathcal{V}) = \text{proj}_{V}(X).
$$
换句话说，条件期望 $Z = E(X|\mathcal{V})$ 是下面最小二乘问题的解
$$
\|Z - X\|^{2}_{L^{2}(\Omega)} = \min_{Y \in V}\|Y - X\|^{2}_{L^{2}(\Omega)},
$$
它是**最 “接近” $X$ 的 $\mathcal{V}$-可测随机变量**。

### 2.6.4 条件期望的诸性质

现在万事俱备，可以介绍条件期望的性质了

> **定理 2.52 （条件期望的诸性质）**
> 1. $a, b$ 是常数，则 $E(aX + bY | \mathcal{V}) = aE(X|\mathcal{V}) + bE(X|\mathcal{V})\quad \text{a.s.}$
> 2. $X$ 是 $\mathcal{V}$-可测的，则 $E(X|\mathcal{V}) = X\quad\text{a.s.}$
> 3. $X$ 是 $\mathcal{V}$-可测的，$XY$ 可积，则 $E(XY|\mathcal{V}) = X \cdot E(Y|\mathcal{V})\quad \text{a.s.}$
> 4. $X$ 和 $\mathcal{V}$ 相互独立，则 $E(X|\mathcal{V}) = E(X)\quad\text{a.s.}$
> 5. $\mathcal{W} \subset \mathcal{V}$，则 $E(X|\mathcal{W}) = E\big( E(X|\mathcal{V})|\mathcal{W} \big) = E\big( E(X|\mathcal{W})|\mathcal{V} \big)\quad \text{a.s.}$
> 6. $X \leqslant Y\quad\text{a.s.} \implies E(X|\mathcal{V}) \leqslant E(Y|\mathcal{V})\quad\text{a.s.}$

**证明.**
（1）和（2）是显然的
（3）首先假设 $X$ 是简单函数，也即 $\displaystyle X = \sum\limits_{i=1}^{m} b_{i}\chi_{B_{i}}$，其中 $B_{i} \in \mathcal{V}， i= 1, \dots, m$，于是$$
\begin{align}
\int_{A} X \cdot E(Y|\mathcal{V}) \, \mathrm dP &= \sum\limits_{i=1}^{m} b_{i} \int_{A \cap B_{i}} E(Y|\mathcal{V}) \, \mathrm dP
\\
&= \sum\limits_{i=1}^{m} b_{i} \int_{A \cap B_{i}} Y \, \mathrm dP & 注意 ~A \cap B_{i} \in \mathcal{V}, ~用性质~1\\
&= \int_{A} XY \, \mathrm dP 
\end{align} 
$$接着可以用简单函数逼近任意的随机变量。最后根据条件期望在 a.s. 意义下的唯一性即可。
（4）它等价于证明对任意 $A \in \mathcal{V}$，都有 $\displaystyle \int_{A} E(X) \, \mathrm dP = \int_{A} X \, \mathrm dP$。这并不难：
$$
\begin{align}
\int_{A}^{} {X} \, \mathrm d{P} &= \int_{\Omega}^{} {\chi_{A}X} \, \mathrm d{P} = E(\chi_{A}X) \\
&= E(\chi_{A})E(X) = E(X)\int_{A}^{} {} \, \mathrm d{P} = E(X)P(A) & X ~和~\mathcal{V}~独立\\
&= \int_{A}^{} {E(X)} \, \mathrm d{P} 
\end{align}
$$
（5）假设 $A \in \mathcal{W}$ 且 $\mathcal{W} \subset \mathcal{V}$。一方面，连用两次性质 2，有$$\begin{align}
\int_{A}^{} {E\big( E(X|\mathcal{V})|\mathcal{W} \big) } \, \mathrm d{P} &= \int_{A}^{} {E(X|\mathcal{V})} \, \mathrm d{P} = \int_{A}^{} {X} \, \mathrm d{P} \\
\end{align}$$这就证明了 $E(X|\mathcal{W}) = E(E(X|\mathcal{V})|\mathcal{W})\quad\text{a.s.}$。另一方面，因为 $E(X|\mathcal{W})$ 是 $\mathcal{W}$-可测的，所以它一定是 $\mathcal{V}$-可测的，因此 $E(E(X|\mathcal{W})|\mathcal{V}) = E(X|\mathcal{W})$。
（6）对任意 $A \in \mathcal{V}$，我们可以这样构造
$$
\int_{A}^{} {E(Y|\mathcal{V}) - E(X|\mathcal{V})} \, \mathrm d{P} = \int_{A}^{} {E(Y-X|\mathcal{V})} \, \mathrm d{P} = \int_{A}^{} {Y - X} \, \mathrm d{P}  \geqslant 0 ,
$$
我们可以取 $A = \{ E(Y|\mathcal{V}) - E(X|\mathcal{V}) \leqslant 0 \}$，这样根据上面的结果只有 $P(A) = 0$（因为根据 $A$ 的定义，上面的积分只能小于等于零）

> **定理 2.53 （条件 Jensen 不等式）**
> 考虑凸函数 $\Phi: \mathbb{R} \rightarrow \mathbb{R}$，且 $E(|\Phi(X)|) < \infty$，则 $$\Phi(E(X|\mathcal{V})) \leqslant E(\Phi(X)|\mathcal{V})$$

**证明.** 仿照前面的做法：
$$
\int_{A}^{} {E(\Phi(X)|\mathcal{V})} \, \mathrm d{P} = \int_{A}^{} {\Phi(X)} \, \mathrm d{P} \geqslant \Phi\left( \int_{A}^{} {X} \, \mathrm d{P}  \right) = \Phi\left( \int_{A}^{} {E(X|\mathcal{V})} \, \mathrm d{P}  \right) \quad\forall\,A \in \mathcal{V}
$$
取 $A = \Omega$，就是我们要证明的结果。

## 2.7 鞅

### 2.7.1 定义

### 2.7.2 鞅的诸不等式



 
