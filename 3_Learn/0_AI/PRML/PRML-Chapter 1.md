
> [!info] 机器学习的基本术语
> 
> * **模式识别**：使用计算机自动发现数据中的规律，并凭借发现的规律用于分析处理数据
> * **机器学习**：复杂任务中数据难以以人工找出普适的规律，因此采用自动的机器学习算法代替人类发现规律
> * **数据集**：一系列（通常是大量）数据构成的集合，通常已知每条数据对应的标签，即**目标向量**
> * **训练集**：用于调节机器学习算法参数的数据集
> * **泛化**：机器学习模型正确预测未见数据的能力
> * **测试集**：用于测试机器学习算法训练后在未见数据上的**泛化**性能的数据集
> * **训练/学习**：机器学习模型在训练集上调节其参数，以最小化误差或其他数值指标的过程
> * **预处理**：为输入机器学习模型而对数据进行的操作，使得训练可以正常运行，并变得更容易、更迅速，或取得更好的结果。
  预处理的例子有去重、补充或处理缺失值、做数据变换、做数据增强等
> * **特征抽取**：在训练前人为对数据做的一些变换，使得后续训练更容易。
  例如手写数据集中在训练前先计算某个像素周围一个小区域内的灰度平均值，和原输入一起喂给机器学习模型。
> * **监督学习**：训练数据样本包含数据和对应的标签，即训练集有正确答案。
  典型的问题有**分类**问题和**回归**问题，分类自不必说，回归指的是预测一个连续的值
> * **无监督学习**：训练数据样本中没有标签，即训练集没有答案。
  典型的问题有**聚类**（看看数据中哪些聚成了一类）、**密度估计**（根据已有的数据，估计数据出现在未被采样到地方的概率）。这些工具一般服务于数据可视化。
> * **强化学习**：智能体（agent）在环境（environment）中做动作（action）并得到奖励（reward），智能体的目的是最大化平均的累计奖励。
>     * *探索*：智能体更多地探索没有去过的地方。
>     * *利用*：智能体更多地走过已被证实高收益的地方。

## 1.1 例子：多项式曲线拟合

假设真实数据位于 $y = \sin(2\pi x)$ 的曲线上。构造一个训练集 $\boldsymbol{X} = [x_{1}, x_{2}, \dots, x_{N}]^{\top}$，对应的观测值为 $\boldsymbol{t} = [t_{1}, t_{2}, \dots, t_{N}]^{\top}$。训练集中的点采样至 $[0, 1]$ 上的均匀分布，观测值为对应的函数值加上一个小噪声：
$$
t_{i} = y(x_{i}) + \epsilon = \sin(2\pi x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^{2})
$$
其中 $\sigma$ 是一个小数。

> [!info] 噪声
> 真实的数据存在噪声。理想条件下，我们假定数据分布在高维空间中的某个“曲面” $\mathcal{M}$ 上，而实际我们采样到的数据往往并非恰好在 $\mathcal{M}$ 上，而是位于包裹着 $\mathcal{M}$ 的一个管道中。

台大李宏毅老师主讲的《机器学习》课程中，开门见山的讲，机器学习的目的是**找一个函数 (find a function)**，当然，函数多种多样，我们要明确**到哪里找这样一个函数**，即备选函数的集合，我们在这个集合里面找我们要找的那一个函数，这个集合叫做**假设空间 (hypothesis space)**。本例中的假设空间是 $M$ 阶多项式集合 $\mathbb{R}[x]_{\leqslant M}$，它里面的元素可以写成
$$
y(x) = w_{0} + w_{1}x + w_{2}x^{2} + \cdots + w_{M}x^{M} = \sum\limits_{j=0}^{M} w_{j}x^{j}
$$
如果我们将 $x^{j}, j \in \{ 0, 1, \dots, M \}$ 视为一个新的元素 $x_{(j)}$，那么 $y(x)$ 就变成了 $y(x_{0}, x_{1}, \dots, x_{M}) = w_{0}x_{0} + x_{1}x_{1} + \cdots + w_{M}x_{M}$，它对 $x_{0}, x_{1}, \dots, x_{M}$ 是线性的。我们将 $x_{j}$ 和 $w_{j}$ 都排布成一个向量，即 $\boldsymbol{x}_{i} = [x_{i}^{(0)}, x_{i}^{(1)}, \dots, x_{i}^{(M)}]^{\top} = [1, x_{i}, \dots, x_{i}^{M}]^{\top} \in \mathbb{R}^{M+1}$，$\boldsymbol{w} = [w_{0}, w_{1}, \dots, w_{M}]^{\top} \in \mathbb{R}^{M+1}$，上面的函数就可以写成两个向量的内积：
$$
y(\boldsymbol{x};\boldsymbol{w}) = \left\langle \boldsymbol{w}, \boldsymbol{x} \right\rangle = \boldsymbol{w}^{\top} \boldsymbol{x} = \sum\limits_{j=0}^{M} w_{j}x^{(j)} = \sum\limits_{j=0}^{M} w_{j}x^{j} 
$$
其中 $y(\boldsymbol{x};\boldsymbol{w})$ 中的 $;\boldsymbol{w}$ 指的是 $\boldsymbol{w}$ 在这个函数中是参数。

现在我们有了假设空间，我们要选一个“最好”的函数，就要定义何为“最好”。我们要预测对应的函数值，这属于**回归**问题。对于回归问题，一个典型的取法是取上面的模型（函数）为所有训练集中数据预测值与真实值之间的平方之和，即所谓的误差函数（损失函数）：
$$
E(\boldsymbol{w}) = \frac{1}{2}\sum\limits_{n=1}^{N} \Big[ y(\boldsymbol{x}_{n}; \boldsymbol{w}) - t_{n} \Big]^{2}
$$
其中前面乘二分之一是为了方便求导计算。直观来讲，$E(\boldsymbol{w})$ 总是非负的，它取零当且仅当全部训练数据预测正确，这符合我们的目标。

接下来我们注意到一个细节，我们尚未决定假设空间中的多项式阶数值 $M$。显然 $M$ 不能太小 —— 如果 $M = 0,1$ 则模型函数总是一条直线，而我们事先知道数据采样来自曲线 $y = \sin(2\pi x)$。$M$ 也不能太大：如果 $M$ 恰好是 $N-1$，则 $\boldsymbol{w}$ 有 $N$ 个分量，这将收敛到对训练集所有数据的 Lagrangre 插值多项式。

> [!important] 欠拟合 (under-fitting)、过拟合 (over-fitting) 和双下降 (double descent)
> 在上面的问题中
> * 若 $M$ 较小，模型复杂度低于数据的复杂度，此时增加 $M$ 会使得训练和测试损失函数下降，这称为 **欠拟合**
> * 若 $M$ 较大，模型会仅仅记住所有训练数据点，从而严重影响未见数据的预测精度，这称为 **过拟合**
> 在几乎每一本经典的机器学习书中，都会有一幅这样的图，横坐标是模型参数量，纵坐标是误差。随着模型参数量增加，训练损失逐渐下降，测试损失先下降在上升，<font color="red">然而这并非故事的全部</font>。最早在 1989 年左右，有研究者发现一些机器学习问题中，出现了当模型参数增加，测试误差**先下降，再上升，再下降**的现象，如下图所示
> <center><img src="Pasted%20image%2020251231154815.png" style="zoom:70%"></center>
> 2019 年，Belkin 等人在论文[^1]中首次提出了双下降这个词，以描述上面的现象。不过话说回来，原来的模型也并非被完全推翻，只需将横轴从“参数量”改成“有效参数量”或是“模型灵活性”即可。






## 1.7 练习
**本章学习目标**

**本章思维导图**

**本章概念地图**


> [!NOTE] 练习 1.1（★☆☆）
> 考虑公式 (1.2) 给出的平方和误差函数，其中函数 $y(r,w)$ 由公式 (1.1) 给出。证明最小化误差函数的系数 $\boldsymbol{w}=\{w_{i}\}$ 由下列线性方程的集合给出
> $$
> \sum_{j=0}^{M} A_{ij}w_j = T_i
> \tag{1.122}
> $$
> 其中
> $$
> \quad A_{ij} = \sum_{n=1}^{N} (x_n)^{i+j}, \quad T_i = \sum_{n=1}^{N} (x_n)^{i} t_{n} \tag{1.123}
> $$
> 这里 $i, j$ 表示元素的下标，而 $(x)^{i}$ 表示 $x$ 的次幂。

多项式拟合中的 (1.2) 可以写为
$$
E(\boldsymbol{w}) = \frac{1}{2} \sum\limits_{n=1}^{N} \Big[ y(x_{n}, \boldsymbol{w}) - t_{n}\Big]^{2} = \frac{1}{2} \sum\limits_{n=1}^{N} \Big[ \boldsymbol{x}_{n}^{\top}\boldsymbol{w} -t_{n} \Big]^{2} 
$$
其中 $\boldsymbol{x}_{n} = [1, x_{n}, x_{n}^{2}, \dots, x_{n}^{M}]^{\top} \in \mathbb{R}^{M+1}$，$M$ 为多项式模型的阶数。现在构造数据矩阵 $\boldsymbol{X}$ 和目标向量 $\boldsymbol{t}$：
$$
\boldsymbol{X} = \begin{bmatrix}
\boldsymbol{x}_{1}^{\top}\\\boldsymbol{x}_{2}^{\top}\\\vdots \\\boldsymbol{x}_{N}^{\top}
\end{bmatrix} = \begin{bmatrix}
1 & x_{1} & x_{1}^{2} & \cdots  & x_{1}^{M}\\
1 & x_{1} & x_{1}^{2} & \cdots  & x_{1}^{M}\\
\vdots & \vdots & \vdots & & \vdots \\
1 & x_{1} & x_{1}^{2} & \cdots  & x_{1}^{M}
\end{bmatrix}, \quad \boldsymbol{t} = \begin{bmatrix}
t_{1}\\t_{2}\\\vdots \\t_{N}
\end{bmatrix} \implies \left\| \boldsymbol{X}\boldsymbol{w} - \boldsymbol{t} \right\|_{2}^{2} = \sum\limits_{n=1}^{N} \Big[ \boldsymbol{x}_{n}^{\top}\boldsymbol{w} - t_{n} \Big]^{2} 
$$
因此损失函数 $E(\boldsymbol{w})$ 可以写为
$$
E(\boldsymbol{w}) = 
$$


> [!NOTE] 练习 1.2（★☆☆）
> 写下能够使由公式 (1.4) 给出的正则化的平方和误差函数取得最小值的系数应该满足的与公式 (1.122) 类似的一组线性方程。

> [!NOTE] 练习 1.3（★★☆）
> 假设我们有三个彩色的盒子：$r$（红色）、$b$（蓝色）、$g$（绿色）。盒子 $r$ 里有 3 个苹果，4 个橘子，3 个酸橙；盒子 $b$ 里有 1 个苹果，1 个橘子，0 个酸橙；盒子 $g$ 里有 3 个苹果，3 个橘子和 4 个酸橙。如果盒子随机被选中的概率为 $p(r)=0.2$，$p(b)=0.2$，$p(g)=0.6$。选择一个水果从盒子中拿走（盒子中选择任何水果的概率都相同），那么选择苹果的概率是多少？如果我们观察到选择的水果实际上是橘子，那么它来自绿色盒子的概率是多少？

> [!NOTE] 练习 1.4（★★☆）
> 考虑一个定义在连续变量 $r$ 上的概率密度 $p_x(c)$，假设我们使用 $c=9(y)$ 做了一个非线性变量变换，从而概率密度变换由公式 (1.27) 给出。通过对公式 (1.27) 取微分，请证明，由于 Jacobian 因子的原因，$y$ 的概率密度最大的位置 $\tilde{y}$ 与 $c$ 的概率密度最大的位置 $\tilde{r}$ 的关系通常不是简单的函数关系 $y=99$。这说明概率密度（与简单的函数不同）的最大值取决于变量的选择。请证明，在线性变换的情况下，最大值位置的变换方式与变量本身的变换方式相同。

> [!NOTE] 练习 1.5（★☆☆）
> 使用定义 (1.38) 证明 $\text{var}f(x)$ 满足公式 (1.39)。

> [!NOTE] 练习 1.6（★☆☆）
> 请证明，如果两个变量 $x$ 和 $y$ 是独立的，那么它们的协方差为零。

> [!NOTE] 练习 1.7（★☆☆）
> 在本练习中，我们证明公式 (1.48) 给出的一元高斯分布的归一化条件。为了证明这一点，我们考虑下面的积分
> $$
> I = \int_{-\infty}^{\infty} \exp\left(-\frac{c^2}{2\sigma^2}\right) \mathrm{d}c \tag{1.124}
> $$
> 这个积分可以这样计算：首先将它的平方写成下面的形式
> $$
> I^2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp\left(-\frac{c^2 + v^2}{2\sigma^2}\right) \mathrm{d}c \, \mathrm{d}v \tag{1.125}
> $$
> 现在使用笛卡尔坐标 $(c,y)$ 到极坐标 $(r,\theta)$ 的坐标变换，然后替换 $u=r^2$。请证明，通过对 $\theta$ 和 $u$ 积分，然后两边取平方根，我们可以得到
> $$
> I = (2\pi\sigma^2)^{1/2} \tag{1.126}
> $$
> 最后，使用这个结果，证明高斯分布 $N(|μ,\sigma^2)$ 是归一化的。

> [!NOTE] 练习 1.8（★★☆）
> 通过使用变量替换，证明由公式 (1.46) 给出的一元高斯分布满足公式 (1.49)。接下来，通过对下面的归一化条件
> $$
> \int N(x|μ,\sigma) \,\mathrm{d}r = 1 \tag{1.127}
> $$
> 两侧关于 $\sigma^2$ 求微分，证明高斯分布满足公式 (1.50)。最后，证明公式 (1.51) 成立。

> [!NOTE] 练习 1.9
> 证明由公式 (1.46) 给出的高斯分布的众数（即最大值）为 $L$。类似地，证明由公式 (1.52) 给出的多元高斯分布的众数为 $\mu$。

> [!NOTE] 练习 1.10
> 假设两个变量 $r$ 和 $z$ 是统计独立的。证明它们的和的均值和方差满足
> $$
> E[xc+2] = E[x] + E[] \tag{1.128}
> $$
> $$
> \text{var}[c+2] = \text{var}[x] + \text{var}[2] \tag{1.129}
> $$

> [!NOTE] 练习 1.11
> 通过令对数似然函数 (1.54) 关于 $μ$ 和 $σ^2$ 的导数等于零，证明公式 (1.55) 和公式 (1.56)。

> [!NOTE] 练习 1.12
> 使用公式 (1.49) 和公式 (1.50) 的结果，证明
> $$
> E[x_nx_m] = μ^2 + I_{nm}\sigma^2 \tag{1.130}
> $$
> 其中 $c_n$ 和 $c_m$ 表示从均值为 $μ$ 方差为 $σ^2$ 的高斯分布中采样的数据点。当 $n=m$ 时，$I_{nm}=1$，否则 $I_{nm}=0$。从而证明了公式 (1.57) 和公式 (1.58) 的结果。

> [!NOTE] 练习 1.13
> 假设高斯分布的方差由公式 (1.56) 进行估计，但是估计时将均值的最大似然估计 $μ_{ML}$ 替换为真实的均值 $μ$。证明，此时对于方差的估计的期望等于真实的方差。

> [!NOTE] 练习 1.14
> 证明任意的方阵的元素 $u$ 都可以写成 $w_j = w + w_S$ 的形式，其中 $w$ 和 $u$ 分别是对称矩阵和反对称矩阵，即对于所有的 $i$ 和 $j$ 都有 $w = w$ 和 $u = -u$。现在考虑 $D$ 维空间高阶多项式中的二阶项，由下式给出
> $$
> \sum_{i=1}^{D} \sum_{j=1}^{D} w_{ij}c_i c_j \tag{1.131}
> $$
> 证明
> $$
> \sum_{i=1}^{D} \sum_{j=1}^{D} w_{ij}c_i c_j = \sum_{i=1}^{D} \sum_{j=1}^{D} w_{ij}c_i c_j \tag{1.132}
> $$
> 从而来自反对称矩阵的贡献消失了。于是，我们看到，不失一般性，系数 $u_i$ 的矩阵可以选择成对称的，并且这个矩阵中并非所有 $D^2$ 个元素都可以独立选取。证明，在矩阵 $u$ 中，独立参数的个数为 $\frac{P(D+1)}{2}$。

> [!NOTE] 练习 1.15
> 在这个练习和下一个练习中，我们研究多项式函数的独立参数的数量与多项式阶数 $M$ 以及输入空间维度 $D$ 之间的关系。首先，我们写下 $D$ 维空间多项式的 $M$ 阶项，形式为
> $$
> \sum_{i_1=1}^{D} \sum_{i_2=1}^{D} \cdots \sum_{i_M=1}^{D} w_{i_1i_2\cdots i_M} c_{i_1}c_{i_2}\cdots c_{i_M} \tag{1.133}
> $$
> 系数 $u_{i_1i_2\cdots i_M}$ 由 $D^M$ 个元素组成，但是独立参数的数量远小于此，因为因子 $c_{i_1}c_{i_2}\cdots c_{i_M}$ 有很多互换对称性。首先证明系数的冗余性可以通过把 $V$ 阶项写成下面的形式的方法消除。
> $$
> \frac{1}{M!} \sum_{i_1=1}^{D} \sum_{i_2=1}^{D} \cdots \sum_{i_M=1}^{D} u_{i_1i_2\cdots i_M} c_{i_1}c_{i_2}\cdots c_{i_M} \tag{1.134}
> $$
> 注意，$ú$ 系数和 $u$ 系数之间的关系不需要显式表示。使用这个结果证明，$M$ 阶项的独立参数的数量 $n(D,M)$ 满足下面的递归关系
> $$
> n(D, M) = \sum_{i=1}^{D} n(i, M-1) \tag{1.135}
> $$
> 接下来，使用归纳法证明下面的结果成立
> $$
> \frac{(i+M-2)!}{(M-1)!(i-1)!} = \frac{(D+M-1)!}{(D-1)!M!} \tag{1.136}
> $$
> 可以这样证明：首先证明 $D=1$ 的情况下，对于任意的 $M$，这个结果成立。证明的过程中会使用 $0! = 1$。然后假设这个结论对于 $D$ 维成立，证明它对于 $D+1$ 维也成立即可。最后，使用之前的两个结果，以及数学归纳法，证明
> $$
> n(D,M) = \frac{(D + M - 1)!}{(D - I)!M!} \tag{1.137}
> $$
> 可以这样证明：首先证明这个结果对于 $M=2$ 且任意的 $D \geq 1$ 成立，这可以通过对比练习 1.14 的结果得出。然后使用公式 (1.135) 和公式 (1.136)，证明，如果结果对于 $M-1$ 阶成立，那么对于 $M$ 阶也成立。

> [!NOTE] 练习 1.16
> 在练习 1.15 中，我们证明了 $D$ 维多项式 $M$ 阶项的独立参数的个数满足公式 (1.135) 给出的关系。我们现在寻找阶数小于等于 $M$ 阶的所有项的独立参数的总数 $V(D,M)$。首先，证明 $N(D,M)$ 满足
> $$
> N(D, M) = \sum_{m=0}^{M} n(D, m) \tag{1.138}
> $$
> 其中 $n(D,m)$ 是 $m$ 阶项的独立参数的数量。现在，使用公式 (1.137) 的结果，以及数学归纳法证明
> $$
> N(D,M) = \frac{(D+M)!}{D!M!} \tag{1.139}
> $$
> 可以这样证明：首先证明结果对于 $M=0$ 以及任意的 $D \geq 1$ 成立，然后假设它对于 $M$ 阶成立，证明它对于 $M+1$ 阶也成立即可。最后，使用下面的 Stirling 近似
> $$
> n! \approx n^n e^{-n} \sqrt{2\pi n} \tag{1.140}
> $$
> 这个近似关系对于大的 $n$ 成立。证明，对于 $D \gg M$，$N(D,M)$ 的增长方式类似于 $D^M$，对于 $M \gg D$，它的增长方式类似于 $M^D$。考虑 $D$ 维的立方（$M=3$）多项式，计算下面两种情形的独立参数的总数：（1）$D=10$ 和（2）$D=100$，这对应于典型的小规模和中规模的机器学习应用问题。

> [!NOTE] 练习 1.17
> Gamma 函数的定义为
> $$
> \Gamma(c) = \int_0^\infty u^{c-1}e^{-u} \mathrm{d}u \tag{1.141}
> $$
> 使用分部积分法，证明 $\Gamma(c+1) = c\Gamma(c)$。并且证明，$\Gamma(1) = 1$，因此当 $c$ 为整数时，$\Gamma(c+1) = c!$。

> [!NOTE] 练习 1.18
> 我们可以使用公式 (1.126) 的结果来推导 $D$ 维空间中单位半径的球体的表面积 $S_D$ 和体积 $V_D$。为了完成这一点，考虑下面的结果。这个结果是通过从笛卡尔坐标系到极坐标系的坐标变换的方式得到的。
> $$
> \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} e^{-x^2} \mathrm{d}x_1 \cdots \mathrm{d}x_D = S_D \int_0^\infty e^{-r^2} r^{D-1} \mathrm{d}r \tag{1.142}
> $$
> 使用 Gamma 函数的定义 (1.141) 以及公式 (1.126)，计算方程的两侧，从而证明
> $$
> S_D = \frac{2\pi^{D/2}}{\Gamma(D/2)} \tag{1.143}
> $$
> 接下来，通过对半径从 0 到 1 进行积分，证明 $D$ 维单位球体的体积为
> $$
> V_D = \frac{S_D}{D} \tag{1.144}
> $$
> 最后，使用结果 $\Gamma(1) = 1$ 和 $\Gamma(3/2) = \sqrt{\pi}/2$，证明对于 $D=2$ 和 $D=3$ 的情形，公式 (1.143) 和公式 (1.144) 就是通常的结果。

> [!NOTE] 练习 1.19
> 考虑 $D$ 维空间的一个半径为 $a$ 的球体和一个同心的边长为 $2a$ 的超立方体，球面与超立方体的每个面的中心接触。通过使用练习 1.18 的结果，证明球与超立方体的体积比为
> $$
> \frac{\text{球的体积}}{\text{超立方体的体积}} = \frac{\pi^{D/2}}{D 2^{D-1} \Gamma(D/2)} \tag{1.145}
> $$
> 接下来使用下面形式的 Stirling 公式
> $$
> \Gamma(x+1) \approx (2\pi)^{1/2} e^{-x} x^{x+1/2} \tag{1.146}
> $$
> 对于 $x \gg 1$ 的情况成立。证明，对于 $D \to \infty$，比值 (1.145) 趋于零。并且证明，超立方体从中心到某个角的距离与从中心到某条边的垂直距离的比值为 $\sqrt{D}$，从而对于 $D \to \infty$，这个比值也趋于无穷大。从这些结果中，我们可以看到，在高维空间中，立方体的大部分体积集中在数量众多的角上，这些角本身有着非常长的"尖刺"！

> [!NOTE] 练习 1.20
> 在本练习中，我们研究高维高斯空间的高斯分布的行为。考虑 $D$ 维空间的一个高斯分布，形式如下
> $$
> p(\boldsymbol{x}) = \frac{1}{(2\pi\sigma^2)^{D/2}} \exp\left(-\frac{\|\boldsymbol{x}\|^2}{2\sigma^2}\right) \tag{1.147}
> $$
> 我们想要找到关于极坐标半径的概率密度，其中方向变量已经被积分出去。为了完成这一点，证明，概率密度在一个半径为 $r$ 且厚度为 $e$ 的球壳上的积分为 $p(r)e$，其中 $e \ll 1$，且
> $$
> p(r) = \frac{S_D}{(2\pi\sigma^2)^{D/2}} r^{D-1} \exp\left(-\frac{r^2}{2\sigma^2}\right) \tag{1.148}
> $$
> 这里，$S_D$ 是 $D$ 维单位球体的表面积。证明，对于大的 $D$ 值，函数 $p(r)$ 有一个驻点位于 $r \approx \sqrt{D}\sigma$ 处。通过考虑 $p(r+\epsilon)$，其中 $\epsilon \ll r$，证明对于大的 $D$ 值
> $$
> p(r+\epsilon) = p(r) \exp\left(-\frac{\epsilon^2}{2\sigma^2}\right) \tag{1.149}
> $$
> 这表明，$r$ 是径向概率密度的最大值点，且远离最大值点 $r$ 时，$p(r)$ 会指数衰减，长度缩放因子为 $\sigma$。我们已经看到，对于大的 $D$ 值，$\sigma \ll r$，因此我们看到大部分的概率质量都集中于大半径的薄球壳上。最后，证明概率密度 $p(\boldsymbol{x})$ 在原点处的值大于在半径 $r$ 处的值，二者的差别是一个值为 $\exp(D/2)$ 的因子。于是我们看到，高维高斯分布的概率质量最大的位置不同于半径上概率密度最大的位置。当我们在后续章节中考虑模型参数的贝叶斯推断时，高维空间中的高斯分布的这个性质将会起重要的作用。

> [!NOTE] 练习 1.21
> 考虑两个非负数 $a$ 和 $b$，证明，如果 $a \leq b$，那么 $a \leq \sqrt{ab}$。使用这个结果证明，如果二分类问题的决策区域被选择为最小化误分类的概率，那么这个概率满足
> $$
> p(\text{误分类}) \leq \int \sqrt{p(x,C_1)p(x,C_2)} \, \mathrm{d}x \tag{1.150}
> $$

> [!NOTE] 练习 1.22
> 给定一个损失矩阵，其元素为 $L_{ki}$，如果对于每个 $c$，我们都选择使公式 (1.81) 取得最小值的类别，那么期望风险会最小。证明，如果损失矩阵为 $L_{kj}=1-I_{kj}$，其中 $I_k$ 是单位矩阵的元素，那么选择类别的方法就变成了选择具有最大后验概率的类别。这种形式的损失矩阵的意义是什么？

> [!NOTE] 练习 1.23
> 对于一般的损失矩阵和一般的类先验概率，推导最小化期望损失的准则。

> [!NOTE] 练习 1.24
> 考虑一个分类问题。这个问题中，把来自类别 $C_k$ 的输入向量分类为类别 $C_j$ 所造成的损失由损失矩阵 $L_{kj}$ 给出。并且，选择拒绝选项所造成的损失为 $\lambda$。找到最小化期望损失的决策准则。证明，当损失矩阵为 $L_{kj}=1-I_{kj}$ 时，这个结果就变成了 1.5.3 节讨论的拒绝准则。$\lambda$ 和拒绝阈值之间的关系是什么？

> [!NOTE] 练习 1.25
> 考虑将一元目标变量 $t$ 的平方和损失函数 (1.87) 推广到多元目标变量 $\boldsymbol{t}$。推广后的形式为
> $$
> E[L(\boldsymbol{t},y(\boldsymbol{x}))] = \iint \|\boldsymbol{y}(\boldsymbol{x})-\boldsymbol{t}\|^2 p(\boldsymbol{x},\boldsymbol{t}) \, \mathrm{d}\boldsymbol{x} \, \mathrm{d}\boldsymbol{t} \tag{1.151}
> $$
> 使用变分法，证明使得这个期望损失取得最小值的函数 $y(\boldsymbol{x})$ 为 $y(\boldsymbol{x}) = E[\boldsymbol{t}|\boldsymbol{x}]$。证明，对于一元目标变量 $t$，这个结果就变成了公式 (1.89) 给出的结果。

> [!NOTE] 练习 1.26
> 通过将公式 (1.151) 中的平方项展开，推导类似于公式 (1.90) 的结果，证明，对于目标变量组成向量 $\boldsymbol{t}$ 的情形，最小化期望平方损失的函数 $y(\boldsymbol{x})$ 仍然是 $\boldsymbol{t}$ 的条件期望。

> [!NOTE] 练习 1.27
> 考虑回归问题的期望损失，损失函数为公式 (1.91) 给出的 $L_q$。写出为了最小化 $E[L_q]$，$y(\boldsymbol{x})$ 必须满足的条件。证明，对于 $q=1$，这个解表示条件中位数，即函数 $y(\boldsymbol{x})$ 使 $t<y(\boldsymbol{x})$ 的概率质量与 $t \geq y(\boldsymbol{x})$ 的概率质量相同。并且证明，对于 $q \to 0$，最小的期望 $L_q$ 误差为条件众数，即函数 $y(\boldsymbol{x})$ 等于最大化 $p(t|\boldsymbol{x})$ 的 $t$ 值。

> [!NOTE] 练习 1.28
> 在 1.6 节，我们介绍了熵 $h(c)$ 的思想，即观察到概率分布为 $p(c)$ 的随机变量 $c$ 的值之后所获得的信息。我们看到，对于独立的变量 $r$ 和 $y$，有 $p(c,y)=p(c)p(y)$，且熵函数是可加的，即 $h(c,y)=h(c)+h(y)$。在这个练习中，我们推导 $h$ 和 $p$ 的函数关系 $h(p)$。首先证明 $h(p^2)=2h(p)$，因此通过数学归纳法，有 $h(p^n)=nh(p)$，其中 $n$ 是正整数。因此，证明 $h(p^m)=(\frac{m}{n})h(p)$，其中 $m$ 也是一个正整数。这表明 $h(p^x)=xh(p)$，其中 $x$ 是一个正有理数。从而根据连续性，这个结果对于 $r$ 是正实数的情形也成立。最后，证明上述结果表明了 $h(p)$ 的形式一定为 $h(p) \propto \ln p$。

> [!NOTE] 练习 1.29
> 考虑一个 $M$ 状态的离散随机变量 $c$，使用公式 (1.115) 给出的 Jensen 不等式证明概率分布 $p(c)$ 的熵满足 $H[c] \leq \ln M$。

> [!NOTE] 练习 1.30
> 计算两个高斯分布 $p(c)=N(c|μ,σ^2)$ 和 $q(c)=N(c|m,s^2)$ 之间的由公式 (1.113) 给出的 Kullback-Leibler 散度。

> [!NOTE] 练习 1.31
> 考虑两个变量 $c$ 和 $y$，联合概率分布为 $p(c,y)$。证明这对变量的微分熵满足
> $$
> H[c,y] \leq H[c] + H[y] \tag{1.152}
> $$
> 当且仅当 $c$ 和 $y$ 统计独立时等号成立。

> [!NOTE] 练习 1.32
> 考虑一个连续向量 $\boldsymbol{x}$，概率分布为 $p(\boldsymbol{x})$，对应的熵为 $H[\boldsymbol{x}]$。假设我们对 $\boldsymbol{x}$ 进行了一个非奇异的线性变换，得到一个新的变量 $\boldsymbol{y} = A\boldsymbol{x}$。证明对应的熵为 $H[\boldsymbol{y}] = H[\boldsymbol{x}] + \ln|\det(A)|$，其中 $\det(A)$ 表示 $A$ 的行列式的值。

> [!NOTE] 练习 1.33
> 假设两个离散随机变量 $r$ 和 $y$ 的条件熵 $H[y|x]$ 为零。证明，对于所有的满足 $p(x)>0$ 的 $x$，变量 $y$ 一定是 $c$ 的函数。换句话说，对于每个 $r$，只有一个 $y$ 的值使得 $p(y|x) \neq 0$。

> [!NOTE] 练习 1.34
> 使用变分法证明公式 (1.108) 之前的泛函的驻点由公式 (1.108) 给出。然后使用限制条件 (1.105)、(1.106) 和 (1.107)，消去拉格朗日乘数，从而证明最大熵的解由高斯分布 (1.109) 给出。

> [!NOTE] 练习 1.35
> 使用公式 (1.106) 和公式 (1.107) 的结果，证明一元高斯分布 (1.109) 的熵为 (1.110)。

> [!NOTE] 练习 1.36
> 一个严格凸函数的定义为：每条弦都位于函数图像上方的函数。证明，这等价于函数的二阶导数为正。

> [!NOTE] 练习 1.37
> 使用定义 (1.111) 以及概率的乘积规则，证明公式 (1.112) 的结果。

> [!NOTE] 练习 1.38
> 使用归纳法，证明从凸函数的不等式 (1.114) 可以推导出公式 (1.115)。

> [!NOTE] 练习 1.39
> 考虑两个变量 $r$ 和 $y$，每个变量只有两个可能的取值。它们的联合概率分布在表 1.3 中给出。计算下面各式的值，画一个图说明这些量之间的关系。
> $$
> H[x] \quad H[y|x] \quad H[x,y] \quad H[y] \quad H[x|y] \quad I[x,y]
> $$
> **表 1.3**：练习 1.39 使用的两个二值变量 $x$ 和 $y$ 的联合概率分布。行表示 $r$ 的值，列表示 $y$ 的值。
> $$
> \begin{array}{c|cc}
>  & y=0 & y=1 \\
> \hline
> x=0 & 1/3 & 1/3 \\
> x=1 & 0 & 1/3
> \end{array}
> $$

> [!NOTE] 练习 1.40
> 使用 Jensen 不等式 (1.115)，其中 $f(c)=\ln c$，证明一组实数的算术平均值永远不小于它们的几何平均值。

> [!NOTE] 练习 1.41
> 使用概率的加和规则和乘积规则，证明互信息 $I(c,y)$ 满足关系 (1.121)。




[^1]: Schaeffer, Rylan; Khona, Mikail; Robertson, Zachary; Boopathy, Akhilan; Pistunova, Kateryna; Rocks, Jason W.; Fiete, Ila Rani; Koyejo, Oluwasanmi (2023-03-24). "[Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle](https://arxiv.org/abs/2303.14151v1)"

