
> [!info] 机器学习的基本术语
> 
> * **模式识别**：使用计算机自动发现数据中的规律，并凭借发现的规律用于分析处理数据
> * **机器学习**：复杂任务中数据难以以人工找出普适的规律，因此采用自动的机器学习算法代替人类发现规律
> * **数据集**：一系列（通常是大量）数据构成的集合，通常已知每条数据对应的标签，即**目标向量**
> * **训练集**：用于调节机器学习算法参数的数据集
> * **泛化**：机器学习模型正确预测未见数据的能力
> * **测试集**：用于测试机器学习算法训练后在未见数据上的**泛化**性能的数据集
> * **训练/学习**：机器学习模型在训练集上调节其参数，以最小化误差或其他数值指标的过程
> * **预处理**：为输入机器学习模型而对数据进行的操作，使得训练可以正常运行，并变得更容易、更迅速，或取得更好的结果. 
  预处理的例子有去重、补充或处理缺失值、做数据变换、做数据增强等
> * **特征抽取**：在训练前人为对数据做的一些变换，使得后续训练更容易. 
  例如手写数据集中在训练前先计算某个像素周围一个小区域内的灰度平均值，和原输入一起喂给机器学习模型. 
> * **监督学习**：训练数据样本包含数据和对应的标签，即训练集有正确答案. 
  典型的问题有**分类**问题和**回归**问题，分类自不必说，回归指的是预测一个连续的值
> * **无监督学习**：训练数据样本中没有标签，即训练集没有答案. 
  典型的问题有**聚类**（看看数据中哪些聚成了一类）、**密度估计**（根据已有的数据，估计数据出现在未被采样到地方的概率）. 这些工具一般服务于数据可视化. 
> * **强化学习**：智能体（agent）在环境（environment）中做动作（action）并得到奖励（reward），智能体的目的是最大化平均的累计奖励. 
>     * *探索*：智能体更多地探索没有去过的地方. 
>     * *利用*：智能体更多地走过已被证实高收益的地方. 

## 1.1 例子：多项式曲线拟合

假设真实数据位于 $y = \sin(2\pi x)$ 的曲线上. 构造一个训练集 $\boldsymbol{X} = [x_{1}, x_{2}, \dots, x_{N}]^{\top}$，对应的观测值为 $\boldsymbol{t} = [t_{1}, t_{2}, \dots, t_{N}]^{\top}$. 训练集中的点采样至 $[0, 1]$ 上的均匀分布，观测值为对应的函数值加上一个小噪声：
$$
t_{i} = y(x_{i}) + \epsilon = \sin(2\pi x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^{2})
$$
其中 $\sigma$ 是一个小数. 

> [!info] 噪声
> 真实的数据存在噪声. 理想条件下，我们假定数据分布在高维空间中的某个“曲面” $\mathcal{M}$ 上，而实际我们采样到的数据往往并非恰好在 $\mathcal{M}$ 上，而是位于包裹着 $\mathcal{M}$ 的一个管道中. 

台大李宏毅老师主讲的《机器学习》课程中，开门见山的讲，机器学习的目的是**找一个函数 (find a function)**，当然，函数多种多样，我们要明确**到哪里找这样一个函数**，即备选函数的集合，我们在这个集合里面找我们要找的那一个函数，这个集合叫做**假设空间 (hypothesis space)**. 本例中的假设空间是 $M$ 阶多项式集合 $\mathbb{R}[x]_{\leqslant M}$，它里面的元素可以写成
$$
y(x) = w_{0} + w_{1}x + w_{2}x^{2} + \cdots + w_{M}x^{M} = \sum\limits_{j=0}^{M} w_{j}x^{j}
$$
如果我们将 $x^{j}, j \in \{ 0, 1, \dots, M \}$ 视为一个新的元素 $x_{(j)}$，那么 $y(x)$ 就变成了 $y(x_{0}, x_{1}, \dots, x_{M}) = w_{0}x_{0} + x_{1}x_{1} + \cdots + w_{M}x_{M}$，它对 $x_{0}, x_{1}, \dots, x_{M}$ 是线性的. 我们将 $x_{j}$ 和 $w_{j}$ 都排布成一个向量，即 $\boldsymbol{x}_{i} = [x_{i}^{(0)}, x_{i}^{(1)}, \dots, x_{i}^{(M)}]^{\top} = [1, x_{i}, \dots, x_{i}^{M}]^{\top} \in \mathbb{R}^{M+1}$，$\boldsymbol{w} = [w_{0}, w_{1}, \dots, w_{M}]^{\top} \in \mathbb{R}^{M+1}$，上面的函数就可以写成两个向量的内积：
$$
y(\boldsymbol{x};\boldsymbol{w}) = \left\langle \boldsymbol{w}, \boldsymbol{x} \right\rangle = \boldsymbol{w}^{\top} \boldsymbol{x} = \sum\limits_{j=0}^{M} w_{j}x^{(j)} = \sum\limits_{j=0}^{M} w_{j}x^{j} 
$$
其中 $y(\boldsymbol{x};\boldsymbol{w})$ 中的 $;\boldsymbol{w}$ 指的是 $\boldsymbol{w}$ 在这个函数中是参数. 

现在我们有了假设空间，我们要选一个“最好”的函数，就要定义何为“最好”. 我们要预测对应的函数值，这属于**回归**问题. 对于回归问题，一个典型的取法是取上面的模型（函数）为所有训练集中数据预测值与真实值之间的平方之和，即所谓的误差函数（损失函数）：
$$
E(\boldsymbol{w}) = \frac{1}{2}\sum\limits_{n=1}^{N} \Big[ y(\boldsymbol{x}_{n}; \boldsymbol{w}) - t_{n} \Big]^{2}
$$
其中前面乘二分之一是为了方便求导计算. 直观来讲，$E(\boldsymbol{w})$ 总是非负的，它取零当且仅当全部训练数据预测正确，这符合我们的目标. 

接下来我们注意到一个细节，我们尚未决定假设空间中的多项式阶数值 $M$. 显然 $M$ 不能太小 —— 如果 $M = 0,1$ 则模型函数总是一条直线，而我们事先知道数据采样来自曲线 $y = \sin(2\pi x)$. $M$ 也不能太大：如果 $M$ 恰好是 $N-1$，则 $\boldsymbol{w}$ 有 $N$ 个分量，这将收敛到对训练集所有数据的 Lagrangre 插值多项式. 

> [!important] 欠拟合 (under-fitting)、过拟合 (over-fitting) 和双下降 (double descent)
> 在上面的问题中
> * 若 $M$ 较小，模型复杂度低于数据的复杂度，此时增加 $M$ 会使得训练和测试损失函数下降，这称为 **欠拟合**
> * 若 $M$ 较大，模型会仅仅记住所有训练数据点，从而严重影响未见数据的预测精度，这称为 **过拟合**
> 在几乎每一本经典的机器学习书中，都会有一幅这样的图，横坐标是模型参数量，纵坐标是误差. 随着模型参数量增加，训练损失逐渐下降，测试损失先下降在上升，<font color="red">然而这并非故事的全部</font>. 最早在 1989 年左右，有研究者发现一些机器学习问题中，出现了当模型参数增加，测试误差**先下降，再上升，再下降**的现象，如下图所示
> <center><img src="Pasted%20image%2020251231154815.png" style="zoom:70%"></center>
> 2019 年，Belkin 等人在论文[^1]中首次提出了双下降这个词，以描述上面的现象. 不过话说回来，原来的模型也并非被完全推翻，只需将横轴从“参数量”改成“有效参数量”或是“模型灵活性”即可. 








[^1]: Schaeffer, Rylan; Khona, Mikail; Robertson, Zachary; Boopathy, Akhilan; Pistunova, Kateryna; Rocks, Jason W.; Fiete, Ila Rani; Koyejo, Oluwasanmi (2023-03-24). "[Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle](https://arxiv.org/abs/2303.14151v1)"

