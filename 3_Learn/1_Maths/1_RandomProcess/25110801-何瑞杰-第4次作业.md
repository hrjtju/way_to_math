> 何瑞杰
> 25110801
> 整理完成讲义22-23页的习题和思考题，以及老师课堂布置的题目
> 截止时间：10 月 14 日中午 1200

## 2.1 Bernoulli 过程
> 1. 考虑两点分布（计数过程）、二项分布、几何分布、Pascal 分布的分布律、均值、方差、矩生成函数

**两点分布**
* $P(X=1)=p, P(X=0) = 1-p$
* $\mathbb{E}[X] = 1 \cdot p +0 \cdot(1-p) = p$
* $D[X] = p \cdot(1-p)^{2} + (1-p) \cdot (0-p)^{2} = p(1-p)$
* $M_{X}(\lambda) = \mathbb{E}[e^{\lambda X}] = pe^{\lambda} - p + 1$

**二项分布**
* 记 $N$ 次独立实验
* $\displaystyle P(X=m) = \binom{n}{m} p^{m} (1-p)^{n-m}$
$$
\begin{align}
\mathbb{E}[X] 
&= \sum\limits_{m=1}^{n} m \cdot \binom{n}{m} p^{m} (1-p)^{n-m}\\
&= \sum\limits_{m=1}^{n} m \cdot \frac{n!}{(n-m)!m!}p^{m}(1-p)^{n-m}\\
&= \sum\limits_{m=1}^{n} \frac{n \cdot (n-1)!}{((n-1)-(m-1))!(m-1)!}p \cdot p^{m-1}(1-p)^{n-m}\\
&= np \sum\limits_{m=1}^{n} \frac{(n-1)!}{((n-1)-(m-1))!(m-1)!} p^{m-1}(1-p)^{(n-1)-(m-1)}\\
&= np.
\end{align}
$$
$$
\begin{align}
\mathbb{E}[X^{2}] 
&= \sum\limits_{m=1}^{n} m^{2} \cdot \binom{n}{m} p^{m} (1-p)^{n-m}\\
&= \sum\limits_{m=1}^{n} (m(m-1)+m) \cdot \frac{n!}{(n-m)!m!}p^{m}(1-p)^{n-m}\\
&= \sum\limits_{m=2}^{n} \frac{n(n-1) \cdot (n-2)!}{((n-2)-(m-2))!(m-2)!}p^{2} \cdot p^{m-2}(1-p)^{n-m} + \mathbb{E}[X]\\
&=np + n(n-1)p^{2} \sum\limits_{m=2}^{n} \frac{ \cdot (n-2)!}{((n-2)-(m-2))!(m-2)!} p^{m-2}(1-p)^{(n-2)-(m-2)}\\
&= np + n(n-1)p^{2}.
\end{align}
$$
因此有 $D[X] = \mathbb{E}[X^{2}] - \mathbb{E}[X]^{2} = np(1-p)$

$$
\begin{align}
M_{X}(\lambda) 
&= \mathbb{E}[e^{\lambda X}] 
    = \sum\limits_{m=0}^{n} e^{m\lambda} \cdot \binom{n}{m} p^{m} (1-p)^{n-m}\\
&= (1-p)^{n} \sum\limits_{m=0}^{n} \binom{n}{m} \left[ \frac{pe^{\lambda}}{1-p} \right]^{m} 1^{n-m}\\
&= (1-p + pe^{\lambda})^{n}
\end{align}
$$
注意，它的矩母函数就是两点分布的矩母函数的 $n$ 次方。

**几何分布**
* $P(X=k) = p(1-p)^{k-1}$

$$
\begin{align}
\mathbb{E}[X] &= \sum\limits_{k=1}^{\infty} kp(1-p)^{k-1} = -p \sum\limits_{k=1}^{\infty} \displaystyle \frac{ \mathrm{d} }{ \mathrm{d}p }  (1-p)^{k}\\
&= -p \displaystyle \frac{ \mathrm{d} }{ \mathrm{d}p } \sum\limits_{k=1}^{\infty} (1-p)^{k} = -p \displaystyle \frac{ \mathrm{d} }{ \mathrm{d}p } \left( \frac{1}{p} - 1 \right) = \frac{1}{p}.
\end{align}
$$

$$
\begin{align}
\mathbb{E}[X^{2}] &= \sum\limits_{k=1}^{\infty} k^{2}p(1-p)^{k-1} = \sum\limits_{k=-1}^{\infty} [k(k+1) - k] p(1-p)^{k-1}\\
&= p\left[ \sum\limits_{k=1}^{\infty} \displaystyle \frac{ \mathrm{d}^{2} }{ \mathrm{d}p^{2} } (1-p)^{k+1} - \sum\limits_{k=1}^{\infty} k (1-p)^{k-1} \right]\\
&= - \frac{1}{p} + p \displaystyle \frac{ \mathrm{d}^{2} }{ \mathrm{d}p^{2} }  \sum\limits_{k=1}^{\infty} (1-p)^{k+1}\\
&= -\frac{1}{p} + p \displaystyle \frac{ \mathrm{d}^{2} }{ \mathrm{d}p^{2} } \left[ \frac{1}{p} - 1 - (1-p) \right] = \frac{2-p}{p^{2}}
\end{align}
$$
因此 $\displaystyle D[X] = E[X^{2}] - \mathbb{E}^{2}[X] = \frac{1-p}{p}$
$$
\begin{align}
M_{X}(\lambda) &= \mathbb{E}[e^{\lambda X}] = \sum\limits_{k=1}^{\infty} e^{\lambda k} p(1-p)^{k-1}\\
&= pe^{\lambda} \sum\limits_{k=0}^{\infty} [e^{\lambda} (1-p)]^{k} = \frac{pe^{\lambda}}{1 - e^{\lambda}(1-p)}
\end{align}
$$

**Pascal 分布**

$\displaystyle P(X = n) = \binom{n-1}{r-1} p^{r} (1-p)^{n-r}$

$\begin{align}\mathbb{E}[X] &= \sum\limits_{n=r}^{\infty} n \binom{n-1}{r-1} p^{r}(1-p)^{n-r}\\&= \sum\limits_{n=r}^{\infty} n \frac{(n-1)!}{(n-r)!(r-1)!} p^{r}(1-p)^{n-r}\\&= r \sum\limits_{n=r}^{\infty} \frac{n!}{(n-r)!r!} p^{r}(1-p)^{[(n+1) - (r+1)]} = \frac{r}{p}\end{align}$

$$
\begin{align}
\mathbb{E}[X^{2}] &= \sum\limits_{n=r}^{\infty} n^{2} \binom{n-1}{r-1} p^{r}(1-p)^{n-r}\\
&= \sum\limits_{n=r}^{\infty}  n^{2}\frac{(n-1)!}{(n-r)!(r-1)!} p^{r}(1-p)^{n-r}\\
&= r(r+1)\sum\limits_{n=r}^{\infty} [n(n+1) - n] \frac{(n-1)!}{(n-r)!(r+1)!} p^{r}(1-p)^{n-r}\\
&= \frac{r(r+1)}{p^{2}} \sum\limits_{n=r}^{\infty} \frac{(n+1)!}{(n-r)!(r+1)!} p^{r+1}(1-p)^{n-r} - \underbrace{ \sum\limits_{n=r}^{\infty} n \cdot \binom{ n-1}{r-1} p^{r}(1-p)^{n-r} }_{ = \,r/p }\\
&=  -\frac{r}{p} + \frac{r(r+1)}{p^{2}}
\end{align} 
$$
$D[X] = \mathbb{E}[X^{2}] - \mathbb{E}^{2}[X] = \displaystyle -\frac{r}{p} + \frac{r(r+1)}{p^{2}} - \frac{r^{2}}{p^{2}} = \frac{r(1-p)}{p^{2}}$
$$
\begin{align}
M_{X}(\lambda) &= \mathbb{E}[e^{\lambda X}] = \sum\limits_{k=0}^{\infty} e^{\lambda k} \binom{k+r-1}{k} p^{r}(1-p)^{k}\\
&= p^{r} \sum\limits_{k=0}^{\infty} \binom{k+r-1}{k} [(1-p)e^{\lambda}]^{k}\\
&= p^{r} [1- (1-p)e^{t}]^{-r}
\end{align}
$$

> 2. 说明 Bernoulli 过程的无记忆性

记 Bernoulli 过程中首次成功的实验序号为 $T$，已经做了 $n$ 次实验。则
$$
P(T = k | T > n) = \frac{P(T = k, T > n)}{P(T > n)} = \frac{p(1-p)^{k-1}}{(1-p)^{n}} = p(1-p)^{k-n-1} = P(T = k-n).
$$
这说明当已知 $n$ 次实验失败后，成功发生于此后第 $k-n$ 次实验的概率等于成功发生于从头开始第 $k-n$ 次实验的概率。因此 Bernoulli 分布具有无记忆性。

> 3. 给定计数条件下，成功发生的位置的分布律；给定某个时间区间内已经成功的次数，问某个时刻成功的分布律

假设这个时间区间内包含了 $N$ 次实验，其中成功 $k$ 次。由于 Bernoulli 过程的无记忆性，我们可以将该时间区间的第一次实验看作是 Bernoulli 过程的第一次实验。在这个时间区间内，第 $m$ 次实验成功的概率为
$$
P(X_{m} = 1) =  \binom{N-1}{k-1}  \Big/ \binom{N}{k} = \frac{k}{n}
$$

> 4. 求 Bernoulli 过程的极限过程

将原来每单位时间做一次实验，改为每单位时间做 $k$ 次实验，每次实验的成功率为 $p/k$。则 $t$ 时间长度范围中发生 $m$ 次实验成功的概率为
$$
P(X = m) = \binom{kt}{m} \left( \frac{p}{k} \right)^{m}\left( 1-\frac{p}{k} \right)^{kt - m}
$$
现在令 $k \rightarrow \infty$，有
$$
\begin{align}
&\, \lim_{ k \to \infty } \binom{kt}{m} \left( \frac{p}{k} \right)^{m}\left( 1-\frac{p}{k} \right)^{kt - m}\\
=&\, \lim_{ k \to \infty } \frac{(kt)!}{(kt - m)! m!} \frac{p^{m}(k-p)^{kt - m}}{k^{kt}}\\
=&\, \lim_{ k \to \infty } \frac{\sqrt{ 2\pi kt }}{\sqrt{ 2\pi(kt -m) }} \cdot \left( \frac{kt}{e} \right)^{kt} \left( \frac{kt-m}{e} \right)^{-(kt-m)} \frac{p^{m}(k-p)^{kt - m}}{m! k^{kt}} & \text{Stirling 公式}\\
=&\, \lim_{ k \to \infty } e^{-m} \cdot \frac{t^{kt}p^{m}(k-p)^{kt-m}}{(kt - m)^{kt - m}m!} = \frac{(pt)^{m}}{m!} e^{-m} \lim_{ k \to \infty } \left[ \frac{t(k-p)}{kt - m} \right]^{kt-m} \\
=&\, \frac{(pt)^{m}}{m!} e^{-m} \lim_{ k \to \infty } \left[ 1 + \frac{m-pt}{n} \right]^{n} = \frac{(pt)^{m}}{m!} e^{-pt}. & n \leftarrow kt - m
\end{align}
$$

> 5. Bernoulli 过程的样本空间是什么？

Bernoulli 过程的样本空间是无限长 $0$-$1$ 序列构成的集合。其中的元素与 $[0, 1]$ 中的二进制小数一一对应。

> 6. Bernoulli 过程的二叉树表示

根节点是空集。每个节点有且仅有两个子节点，其中一个是 $1$，另一个是 $0$。

## 2.2  Poisson 过程

> 1. 考虑一个独立同分布的随机变量 $X_{1}, X_{2}, \dots$，其中第 $i$ 个到达间隔 $X_{i}$ 是由抛硬币的结果决定 的。对于 $x ∈ \{1, 2\}$，有 $\displaystyle P(X_{i} = x) = \frac{1}{2}$ 。由 $X_n$，$n ≥ 1$ 定义的到达过程是更新过程。

我们需要证明 $X_{n}$ 是独立同分布的。由上面的描述，显然成立，因为每个 $X_{n}$ 都由互不相关的投掷硬币结果决定，而后者的分布列为 $\displaystyle P(X_{i} = 1) = \frac{1}{2}, P(X_{i} = 1) = \frac{1}{2}$。

> 2. 考虑箱子里最初有 1 个白球，1 个黑球。每次取出一个，然后放回，同时添 1 个同色球。设单 位时间做一次试验，记摸出白球为一到达事件。讨论该过程是否是更新过程。

注意每次抽出后都会改变箱子中的球数，且该操作不可逆。历史一定会影响下一次到达间隔的时间长度。因此这不是到达过程。

> 3. 考虑 Poisson 过程中给定时间 $t$，距离下次到达的时间 $Z$ 的边缘分布。

![[Pasted image 20251013211217.png|350]]
设 $t$ 时刻的前一次到达是第 $n = N(t)$ 次，该次到达的时间为 $\tau$。

$$
\begin{align}
P(Z > s) &= \sum\limits_{n=0}^{\infty} \int_{0}^{+\infty} P(Z>s | N(t) = n, S_{N(t)} = \tau)P(N(t) = n, S_{N(t)} = \tau) \, \mathrm{d} \tau\\
&= \sum\limits_{n=0}^{\infty} \int_{0}^{+\infty} P(X_{n+1} > s + t - \tau) P(N(t) = n, S_{N(t)} = \tau)\, \mathrm{d} \tau\\
&= P(X_{n+1} > s + t - \tau) P(X_{n+1} > t - \tau) = e^{-\lambda s}.\\
\end{align}
$$

