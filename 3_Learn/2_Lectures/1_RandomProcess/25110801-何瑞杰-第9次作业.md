> 何瑞杰
> 25110801
> 
>1. 完成老师课堂上布置的习题
>
> <font color="red">25/11/18 12:00截止</font>

---

> ## Optimal Selection Strategy Problem
> A collection of $N≥2$ objects is observed randomly and sequentially one at a time. The observer may either select the current object observed, in which case the selection process is terminated, or reject the object and proceed to observe the next. The observer can rank each object relative to those already observed, and the objective is to maximize the probability of selecting the "best" object according to some criterion. It is assumed that no two objects can be judged to be equal.
> 
> Let $r^∗$ be the smallest positive integer $r$ such that
> $$\displaystyle \frac{1}{N-1}​+\frac{1}{N-2}​+⋯+\frac{1}{r}​≤1.$$
> Show that an optimal policy requires that the first r∗objects be observed. If the r∗th object has rank 1 relative to the others already observed, it should be selected; otherwise, the observation process should be continued until an object of rank 1 relative to those already observed is found.
> 
> **Hint**
> We assume that, if the $r$-th object has rank $1$ relative to the previous $r−1$ objects, then the probability that it is best is $r/N$. For $k≥r^∗$, let $J_{k} ​(0)$ be the maximal probability of finding the best object (assuming $k$ objects have been selected and the $k$-th object is not best relative to the previous $k−1$ objects). Show that
> $$J_{k}(0) = \frac{k}{N} \left( \frac{1}{N-1} + \cdots +\frac{1}{k} \right)$$

* $x_{i} \in \{ 0, 1, T \}$ be the system state, which means whether to accept the current candidate.
* $u_{k} \in \{ 0, 1 \}$ means whether to terminate the process.
* $J_{k}(u_{k})$ be the reward function
	*  $J_{k}(0)$ be the maximum probability of finding the best object assuming $k$ objects have been selected and the $k$-th object is not best relative to the previous $k−1$ objects;
	* $J_{k}(1)$ be the maximum probability of finding the best object at $k$.

Step 1. Define the transition map to be 
$$
\begin{align}
P_{i,j}(u) &= P(x_{k+1}=j | x_{k}=i, u_{k}=u) = \begin{cases}
1 & x_{k} = T, x_{k+1} = T\text{ (temrinated)}\\
1 & x_{k} \neq T, x_{k+1} = T\text{ (temrinated)}\\
\frac{1}{k+1} & x_{k} \neq T, u_{k} = 0, x_{k+1} = 1\text{ (the next is the best)}\\
\frac{k}{k+1} & x_{k} \neq T, u_{k} = 0, x_{k+1} = 0\text{ (the next is NOT the best)}
\end{cases}
\end{align}
$$

Step 2. Calculating maximum probability
* $J_{N}(0) = 0$，$J_{N}(1) = 1$ since we have already exausted all possible candidates.
*  $J_{k}(0)$ is the maximum of termination v.s. contineued exploration at $k+1$: $\displaystyle J_{k}(0) = \max \left\{J_{k+1}(T), \mathbb{E}\left[ J_{k+1}(x_{k+1}) + \frac{k}{k+1} J_{k+1}(0)\right]\right\} = \frac{1}{k+1}J_{k+1}(1) + \frac{k}{k+1}J_{k+1}(0)$ since terminating may result to zero reward as probability of selecting the best object is zero
* $J_{k}(1)$ have the same pattern with $J_{k}(0)$, it is the maximum of probability of reporting the best in the selected ones before termination or continue to explore at time $k+1$: $\displaystyle J_{k}(1) = \max\left[ \frac{k}{N} + J_{k+1}(T), \frac{1}{k+1}J_{k+1}(1)+\frac{k}{k+1}J_{k+1}(0) \right] = \max \left[ \frac{k}{N}, J_{k}(0) \right]$
when  $\displaystyle \frac{k}{N} > J_{k}(0)$，$u_{k} = 1$, and thus $\displaystyle J_{k}(0) = \frac{1}{k+1}\max\left\{ \frac{k+1}{N},J_{k+1}(0) \right\} + \frac{k}{k+1}J_{k+1}(0)$, we can prove by induction that
$$
J_{k}(0) = \frac{k}{N} \left( \frac{1}{N-1} + \frac{1}{N-2} + \cdots + \frac{1}{k} \right) = \frac{k}{N} \sum\limits_{i=k}^{N-1} \frac{1}{i}
$$
Therefore we have the optimal stopping time $\displaystyle r^{*}=\max\limits_{k}~\left[ \frac{1}{N-1} + \frac{1}{N-2} + \cdots + \frac{1}{k} \leqslant 1 \right]$.

