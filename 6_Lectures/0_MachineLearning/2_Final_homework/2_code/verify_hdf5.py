#!/usr/bin/env python3
"""
é€‚é…æ‚¨ä¿®æ”¹åçš„è½¬æ¢ä»£ç çš„éªŒè¯å·¥å…·
æ”¯æŒtrain/val/testä¸‰ä»½æ•°æ®ï¼Œtesté›†ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ç»“æ„
"""

import os
import h5py
import nibabel as nib
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
import json
import argparse
import logging
import sys

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class HDF5ValidatorV2:
    """é€‚é…æ‚¨è½¬æ¢é€»è¾‘çš„éªŒè¯å™¨"""
    
    def __init__(self, h5_base_path: str, original_json_path: str, original_base_path: str):
        self.h5_base_path = Path(h5_base_path)
        self.original_json_path = Path(original_json_path)
        self.original_base_path = Path(original_base_path)
        
        with open(self.original_json_path, 'r') as f:
            self.dataset_info = json.load(f)
    
    def get_expected_files(self) -> List[Tuple[str, str, str]]:
        """
        æ ¹æ®æ‚¨çš„è½¬æ¢é€»è¾‘ç”Ÿæˆé¢„æœŸæ–‡ä»¶åˆ—è¡¨
        è¿”å›: [(split, file_type, relative_path), ...]
        """
        expected_files = []
        
        # å¤„ç† train å’Œ val (itemæ˜¯å­—å…¸)
        for split in ['train', 'val']:
            if split not in self.dataset_info:
                continue
            for item in self.dataset_info[split]:
                # å›¾åƒæ–‡ä»¶
                img_path = item['image'].replace('.nii.gz', '.h5')
                expected_files.append((split, 'image', img_path))
                
                # æ ‡ç­¾æ–‡ä»¶
                if 'label' in item:
                    label_path = item['label'].replace('.nii.gz', '.h5')
                    expected_files.append((split, 'label', label_path))
        
        # å¤„ç† test (itemæ˜¯å­—ç¬¦ä¸²)
        split = 'test'
        if split in self.dataset_info:
            for item in self.dataset_info[split]:
                # item ç›´æ¥æ˜¯æ–‡ä»¶å
                img_path = item.replace('.nii.gz', '.h5')
                expected_files.append((split, 'image', img_path))
        
        return expected_files
    
    def level1_file_integrity(self) -> Dict[str, List[str]]:
        """ä¸€çº§éªŒè¯ï¼šæ–‡ä»¶å®Œæ•´æ€§å’Œå¯è¯»å–æ€§"""
        logger.info("=" * 60)
        logger.info("ã€ä¸€çº§éªŒè¯ã€‘æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥")
        
        corrupted_files = {'train': {'image': [], 'label': []},
                          'val': {'image': [], 'label': []},
                          'test': {'image': []}}
        
        expected_files = self.get_expected_files()
        total = len(expected_files)
        success = 0
        
        for split, file_type, rel_path in expected_files:
            file_path = self.h5_base_path / rel_path
            if self._check_single_file(file_path, split, file_type):
                success += 1
            else:
                corrupted_files[split][file_type].append(str(rel_path))
        
        # ç»Ÿè®¡ç»“æœ
        logger.info(f"æ£€æŸ¥æ–‡ä»¶æ€»æ•°: {total}")
        logger.info(f"æˆåŠŸ: {success}")
        logger.info(f"å¤±è´¥: {total - success}")
        logger.info(f"æˆåŠŸç‡: {success/total*100:.2f}%")
        
        # è¯¦ç»†æŠ¥å‘Š
        for split in ['train', 'val', 'test']:
            for ftype in corrupted_files[split]:
                if corrupted_files[split][ftype]:
                    logger.warning(f"{split}é›† {ftype}ç¼ºå¤±/æŸå: {len(corrupted_files[split][ftype])}ä¸ª")
        
        return corrupted_files
    
    def _check_single_file(self, file_path: Path, split: str, file_type: str) -> bool:
        """æ£€æŸ¥å•ä¸ªHDF5æ–‡ä»¶"""
        try:
            if not file_path.exists():
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            with h5py.File(file_path, 'r') as f:
                if 'data' not in f:
                    logger.error(f"âŒ ç¼ºå°‘'data' dataset: {file_path}")
                    return False
                
                data = f['data']
                # éªŒè¯å±æ€§
                for attr in ['affine', 'original_shape', 'original_dtype']:
                    if attr not in data.attrs:
                        logger.error(f"âŒ ç¼ºå°‘å±æ€§ {attr}: {file_path}")
                        return False
                
                # éªŒè¯æ•°æ®å¯è¯»å–
                test_slice = tuple(slice(0, min(5, s)) for s in data.shape)
                _ = data[test_slice]
                
            logger.debug(f"âœ… {split}/{file_type}: {file_path.name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶æŸå {file_path}: {str(e)}")
            return False
    
    def level2_data_consistency(self, num_samples: int = 20) -> Dict[str, List[str]]:
        """äºŒçº§éªŒè¯ï¼šæ•°æ®ä¸€è‡´æ€§ï¼ˆä»…æŠ½æ ·trainé›†ï¼‰"""
        logger.info("=" * 60)
        logger.info(f"ã€äºŒçº§éªŒè¯ã€‘æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆæŠ½æ ·{num_samples}ä¸ªï¼‰")
        
        mismatches = {'image': [], 'label': []}
        
        # åªéªŒè¯trainé›†ï¼ˆç”¨æˆ·ä»£ç ä¸­ä¸»è¦è½¬æ¢é€»è¾‘ï¼‰
        if 'train' not in self.dataset_info:
            logger.warning("trainé›†ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸€è‡´æ€§éªŒè¯")
            return mismatches
        
        # éšæœºæŠ½æ ·
        train_items = self.dataset_info['train']
        if not train_items:
            return mismatches
        
        sample_indices = np.random.choice(len(train_items), 
                                         size=min(num_samples, len(train_items)), 
                                         replace=False)
        
        for idx in sample_indices:
            item = train_items[idx]
            logger.info(f"\néªŒè¯æ ·æœ¬: {item['image']}")
            
            # éªŒè¯å›¾åƒ
            original_img = self.original_base_path / item['image']
            h5_img = self.h5_base_path / item['image'].replace('.nii.gz', '.h5')
            if not self._compare_data(original_img, h5_img, is_label=False):
                mismatches['image'].append(item['image'])
            
            # éªŒè¯æ ‡ç­¾
            if 'label' in item:
                original_label = self.original_base_path / item['label']
                h5_label = self.h5_base_path / item['label'].replace('.nii.gz', '.h5')
                if not self._compare_data(original_label, h5_label, is_label=True):
                    mismatches['label'].append(item['label'])
        
        # æ€»ç»“
        if not mismatches['image'] and not mismatches['label']:
            logger.info("âœ… æ‰€æœ‰æŠ½æ ·æ•°æ®ä¸åŸå§‹æ•°æ®ä¸€è‡´")
        else:
            logger.error(f"âŒ ä¸ä¸€è‡´æ–‡ä»¶: å›¾åƒ{len(mismatches['image'])}ä¸ª, æ ‡ç­¾{len(mismatches['label'])}ä¸ª")
        
        return mismatches
    
    def _compare_data(self, original_path: Path, h5_path: Path, is_label: bool) -> bool:
        """å¯¹æ¯”åŸå§‹nii.gzå’ŒHDF5æ•°æ®"""
        try:
            # åŠ è½½åŸå§‹æ•°æ®
            nib_data = nib.load(original_path).get_fdata()
            if is_label:
                nib_data = nib_data.astype(np.int64)
            else:
                nib_data = nib_data.astype(np.float32)
            
            # åŠ è½½HDF5æ•°æ®
            with h5py.File(h5_path, 'r') as f:
                h5_data = f['data'][:]
            
            # éªŒè¯å½¢çŠ¶
            if nib_data.shape != h5_data.shape:
                logger.error(f"  âŒ å½¢çŠ¶ä¸åŒ¹é…: {original_path.name}")
                return False
            
            # éªŒè¯æ•°å€¼
            if is_label:
                if not np.array_equal(nib_data, h5_data):
                    logger.error(f"  âŒ æ ‡ç­¾æ•°å€¼ä¸åŒ¹é…: {original_path.name}")
                    return False
            else:
                if not np.allclose(nib_data, h5_data, rtol=1e-5, atol=1e-6):
                    max_diff = np.max(np.abs(nib_data - h5_data))
                    logger.error(f"  âŒ å›¾åƒæ•°å€¼ä¸åŒ¹é…: {original_path.name}, æœ€å¤§å·®å¼‚: {max_diff:.2e}")
                    return False
            
            # é™„åŠ ä¿¡æ¯
            if is_label:
                unique_vals = np.unique(h5_data)
                logger.info(f"  âœ… æ ‡ç­¾å€¼åˆ†å¸ƒ: {unique_vals}")
            else:
                ct_range = (h5_data.min(), h5_data.max())
                logger.info(f"  âœ… CTå€¼èŒƒå›´: [{ct_range[0]:.1f}, {ct_range[1]:.1f}] HU")
            
            return True
            
        except Exception as e:
            logger.error(f"  âŒ å¯¹æ¯”å¤±è´¥ {original_path.name}: {str(e)}")
            return False
    
    def level3_functional_test(self, patch_shape: Tuple[int, int, int] = (128, 128, 128)) -> bool:
        """ä¸‰çº§éªŒè¯ï¼šDataloaderåŠŸèƒ½æµ‹è¯•"""
        logger.info("=" * 60)
        logger.info("ã€ä¸‰çº§éªŒè¯ã€‘DataloaderåŠŸèƒ½æµ‹è¯•")
        
        try:
            # å‡è®¾æ‚¨çš„Datasetç±»åœ¨å½“å‰ç›®å½•æˆ–å·²å®‰è£…
            sys.path.append(os.path.dirname(__file__))
            from HDF5MSDLungPatchDataset import HDF5MSDLungPatchDataset
            
            # æµ‹è¯•trainæ¨¡å¼
            logger.info("æµ‹è¯•trainæ¨¡å¼...")
            train_dataset = HDF5MSDLungPatchDataset(
                h5_base_path=str(self.h5_base_path),
                json_path=str(self.original_json_path),
                train=True,
                patch_shape=patch_shape,
                stride_shape=patch_shape,
                cache_size=2
            )
            
            if len(train_dataset) == 0:
                logger.error("âŒ train_datasetä¸ºç©º")
                return False
            
            logger.info(f"âœ… train_datasetåˆ›å»ºæˆåŠŸï¼Œå…±{len(train_dataset)}ä¸ªpatches")
            
            # æµ‹è¯•testæ¨¡å¼
            logger.info("æµ‹è¯•testæ¨¡å¼...")
            test_dataset = HDF5MSDLungPatchDataset(
                h5_base_path=str(self.h5_base_path),
                json_path=str(self.original_json_path),
                train=False,
                patch_shape=patch_shape,
                stride_shape=patch_shape,
                cache_size=1
            )
            
            if len(test_dataset) == 0:
                logger.error("âŒ test_datasetä¸ºç©º")
                return False
            
            logger.info(f"âœ… test_datasetåˆ›å»ºæˆåŠŸï¼Œå…±{len(test_dataset)}ä¸ªpatches")
            
            # æµ‹è¯•åŠ è½½å•ä¸ªæ ·æœ¬
            logger.info("æµ‹è¯•patchåŠ è½½...")
            for split_name, dataset in [('train', train_dataset), ('test', test_dataset)]:
                # ä»å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾å„å–ä¸€ä¸ªæ ·æœ¬
                test_indices = [0, len(dataset)//2, len(dataset)-1]
                
                for idx in test_indices:
                    try:
                        if split_name == 'train':
                            img_patch, label_patch = dataset[idx]
                            # éªŒè¯å½¢çŠ¶
                            if img_patch.shape != patch_shape or label_patch.shape != patch_shape:
                                logger.error(f"âŒ {split_name} patchå½¢çŠ¶é”™è¯¯")
                                return False
                        else:
                            img_patch = dataset[idx]
                            if img_patch.shape != patch_shape:
                                logger.error(f"âŒ {split_name} patchå½¢çŠ¶é”™è¯¯")
                                return False
                        
                        logger.debug(f"  {split_name}[{idx}]åŠ è½½æˆåŠŸ")
                        
                    except Exception as e:
                        logger.error(f"âŒ åŠ è½½{split_name}[{idx}]å¤±è´¥: {str(e)}")
                        return False
            
            logger.info("âœ… æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    
    def generate_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´éªŒè¯æŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info("ã€ç”ŸæˆéªŒè¯æŠ¥å‘Šã€‘")
        
        report = []
        report.append("=" * 60)
        report.append("MSD Lung HDF5è½¬æ¢éªŒè¯æŠ¥å‘Š")
        report.append("=" * 60)
        
        # æ‰§è¡Œä¸‰çº§éªŒè¯
        corrupted = self.level1_file_integrity()
        mismatches = self.level2_data_consistency(num_samples=5)
        functional_ok = self.level3_functional_test()
        
        # æŠ¥å‘Šä¸€çº§éªŒè¯ç»“æœ
        report.append("\nã€ä¸€çº§éªŒè¯ã€‘æ–‡ä»¶å®Œæ•´æ€§")
        total_corrupted = sum(len(v) for split in corrupted.values() for v in split.values())
        if total_corrupted == 0:
            report.append("âœ… é€šè¿‡ï¼šæ‰€æœ‰æ–‡ä»¶å®Œæ•´ä¸”å¯è¯»å–")
        else:
            report.append(f"âŒ å¤±è´¥ï¼šå…±{total_corrupted}ä¸ªæ–‡ä»¶ç¼ºå¤±æˆ–æŸå")
        
        # æŠ¥å‘ŠäºŒçº§éªŒè¯ç»“æœ
        report.append("\nã€äºŒçº§éªŒè¯ã€‘æ•°æ®ä¸€è‡´æ€§")
        if not mismatches['image'] and not mismatches['label']:
            report.append("âœ… é€šè¿‡ï¼šæŠ½æ ·æ•°æ®ä¸åŸå§‹æ•°æ®ä¸€è‡´")
        else:
            report.append(f"âŒ å¤±è´¥ï¼š{len(mismatches['image'])}ä¸ªå›¾åƒ, {len(mismatches['label'])}ä¸ªæ ‡ç­¾ä¸åŒ¹é…")
        
        # æŠ¥å‘Šä¸‰çº§éªŒè¯ç»“æœ
        report.append("\nã€ä¸‰çº§éªŒè¯ã€‘åŠŸèƒ½æµ‹è¯•")
        if functional_ok:
            report.append("âœ… é€šè¿‡ï¼šDataloaderå¯æ­£å¸¸å·¥ä½œ")
        else:
            report.append("âŒ å¤±è´¥ï¼šDataloaderåŠ è½½å¼‚å¸¸")
        
        # ç»¼åˆç»“è®º
        report.append("\n" + "=" * 60)
        overall_pass = (total_corrupted == 0 and 
                       not mismatches['image'] and 
                       not mismatches['label'] and 
                       functional_ok)
        if overall_pass:
            report.append("ğŸ‰ ç»¼åˆç»“æœï¼šéªŒè¯é€šè¿‡ï¼ŒHDF5æ•°æ®å®Œå…¨å¯ç”¨")
        else:
            report.append("âš ï¸  ç»¼åˆç»“æœï¼šéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é—®é¢˜")
        report.append("=" * 60)
        
        report_str = "\n".join(report)
        logger.info(report_str)
        return report_str


def main():
    parser = argparse.ArgumentParser(description='éªŒè¯HDF5è½¬æ¢ç»“æœï¼ˆé€‚é…æ‚¨çš„è½¬æ¢è„šæœ¬ï¼‰')
    parser.add_argument('--h5_base_path', type=str, required=True, help='HDF5æ ¹ç›®å½•')
    parser.add_argument('--original_base_path', type=str, required=True, help='åŸå§‹æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--json_path', type=str, default=None, help='dataset.jsonè·¯å¾„')
    parser.add_argument('--level', type=int, choices=[1, 2, 3], default=3, help='éªŒè¯çº§åˆ«')
    
    args = parser.parse_args()
    
    if args.json_path is None:
        args.json_path = os.path.join(args.original_base_path, 'dataset.json')
    
    validator = HDF5ValidatorV2(args.h5_base_path, args.json_path, args.original_base_path)
    
    # æ ¹æ®çº§åˆ«æ‰§è¡ŒéªŒè¯
    if args.level >= 1:
        validator.level1_file_integrity()
    
    if args.level >= 2:
        validator.level2_data_consistency(num_samples=5)
    
    if args.level >= 3:
        validator.level3_functional_test()
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    report = validator.generate_report()
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(args.h5_base_path, 'validation_report_v2.txt')
    with open(report_path, 'w') as f:
        f.write(report)
    
    logger.info(f"\néªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


if __name__ == '__main__':
    main()