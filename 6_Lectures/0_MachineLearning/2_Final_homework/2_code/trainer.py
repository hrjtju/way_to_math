"""
Original code is generated by **Kimi AI**

mofified by Ruijie HE according to code on website
- https://github.com/wolny/pytorch-3dunet/blob/master/pytorch3dunet/unet3d/trainer.py
"""

import importlib
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from pathlib import Path
from typing import Dict, Any, Callable, Optional, TypeAlias
import torchvision
import yaml
import wandb
from tqdm import tqdm
import nibabel as nib
from collections import defaultdict
import matplotlib.pyplot as plt

import transforms
from transforms import Compose
from models import get_model
import losses
import datasets

NNModel: TypeAlias = torch.nn.Module | Callable[[torch.Tensor], torch.Tensor]

def filter_name(d: dict) -> dict:
    return {k:v for (k,v) in d.items() if "name" not in k}

class SegTrainer:
    """
    Universal Trainer Framework for 2D/3D Segmentation
    """
    
    def __init__(self, config_path: str):
        # read yaml file
    
        with open(config_path, 'r') as f:
            self.cfg = yaml.safe_load(f)
        
        # setting training device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # training status 
        self.epoch = 0
        self.best_metric = 0.0
        self.patience_counter = 0
        
        # model checkpoint saving
        self.save_dir = Path(self.cfg['logging']['save_dir'])
        self.save_dir.mkdir(exist_ok=True)
        
        # logging
        if self.cfg['logging']['use_wandb']:
            wandb.init(project=self.cfg['logging']['project_name'], 
                      config=self.cfg, name=self.cfg['logging']['run_name'])
    
        # add neural network
        self.network: NNModel = get_model(self.cfg["model"])
        print(f"Network Params: {sum(p.numel() for p in self.network.parameters()):,}")
    
        self.optimizer = torch.optim.AdamW(**self.cfg["optimizer"], params=self.network.parameters())
    
        self.scheduler = getattr(torch.optim.lr_scheduler, self.cfg["lr_scheduler"]["name"])(**filter_name(self.cfg["lr_scheduler"]), optimizer = self.optimizer) 
    
        self.loss_fn = getattr(losses, self.cfg["loss"]["name"])(**filter_name(self.cfg["loss"]))

        # setting up datasets
        self.loaders_cfg = self.cfg["loaders"]
        self.slice_builder = ...
        self.transforms = {
            "train": self.get_transform(self.loaders_cfg["train"]["transformer"]),
            "val": self.get_transform(self.loaders_cfg["val"]["transformer"]),
        }
        self.dataset_cfg = self.loaders_cfg["dataset"]
        self.dataset_train = getattr(datasets, self.dataset_cfg["name"])(**filter_name(self.dataset_cfg), 
                                                                         train=True, 
                                                                         transform=self.transforms["train"],
                                                                         slice_builder_config=self.loaders_cfg["train"]["slice_builder"])
        self.dataloader_train = DataLoader(self.dataset_train, batch_size=self.loaders_cfg["batch_size"])
        
    def get_transform(self, cfg: dict):
        """
        parse yaml config transform to transform for train loader and valid loader. 
        """
        
        raw_tf = {}
        
        for (k,v) in cfg.items():
            transforms_ls = []
            for tr in v:
                if len(tr) == 1:
                    transforms_ls.append(
                        getattr(transforms, tr["name"])()
                    )
                else:
                    transforms_ls.append(
                        getattr(transforms, tr["name"])(**filter_name(tr))
                    )
                
            tf = Compose(transforms=transforms_ls)
            raw_tf[k] = tf
        
        return raw_tf
    
    def train_epoch(self) -> Dict[str, float]:
        """train ONE epoch"""
        self.network.train()
        metrics = defaultdict(list)
        
        pbar = tqdm(self.dataloader_train, desc=f"Train Epoch {self.epoch}")
        for batch in pbar:
            # move training data and label to device
            image = batch[0].to(self.device, dtype=torch.float32)  # (B, 1, D, H, W)
            target = batch[1].to(self.device, dtype=torch.float32)    # (B, D, H, W)
            
            target, target_border = target[:, :1, ...], target[:, 1:, ...]
            
            # prediction
            self.optimizer.zero_grad()
            pred, logits = self.network(image, return_logits=True)  # output shape same as input
            
            # compute loss
            loss = self.loss_fn(logits, target)
            
            # backward pass
            loss.backward()
            if 'grad_clip' in self.cfg['training']:
                torch.nn.utils.clip_grad_norm_(
                    self.network.parameters(), 
                    self.cfg['training']['grad_clip']
                )
            self.optimizer.step()
            
            # 指标
            metrics['loss'].append(loss.item())
            
            pbar.set_postfix({'loss': loss.item()})
        
        return {k: np.mean(v) for k, v in metrics.items()}
    
    def validate(self) -> Dict[str, float]:
        """
        test or validate on validation set
        """
        self.network.eval()
        metrics = defaultdict(list)
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"Val Epoch {self.epoch}")
            for batch in pbar:
                image = batch['image'].to(self.device, dtype=torch.float32)
                target = batch['target'].to(self.device, dtype=torch.long)
                
                # forward pass
                pred = self.network(image)
                
                # compute loss
                loss = self.loss_fn(pred, target)
                metrics['loss'].append(loss.item())
                
                pbar.set_postfix({'loss': loss.item()})
        
        return {k: np.mean(v) for k, v in metrics.items()}
    
    def fit(self):
        """
        main training loop
        """
        
        print(f"Starting training [{self.cfg['training']['num_epochs']}] epochs")
        
        for epoch in range(self.cfg['training']['num_epochs']):
            self.epoch = epoch
            
            # train ONE epoch
            train_metrics = self.train_epoch(self.dataloader_train)
            
            # validation
            # val_metrics = self.validate(val_loader)
            
            # update LR scheduler
            if self.scheduler:
                self.scheduler.step()
            
            # add logging info
            self._log_metrics({**train_metrics, 
                            #    **val_metrics
                               })
            
            # save checkpoints
            self._save_checkpoint(train_metrics)
            
            # Early stopping
            if self._check_early_stopping(train_metrics):
                print(f"Early stopping triggered at epoch {epoch}")
                break
    
    def _log_metrics(self, metrics: Dict[str, float]):
        """
        logging
        """
        print(f"Epoch {self.epoch}: " + 
              f"Train Loss: {metrics['loss']:.4f}, " +
              f"Val Dice: {metrics['dice']:.4f}")
        
        if self.cfg['logging']['use_wandb']:
            wandb.log(metrics, step=self.epoch)
    
    def _save_checkpoint(self, val_metrics: Dict[str, float]):
        """
        save latest & best model
        """
        current_dice = val_metrics['dice']
        
        # save latest
        torch.save({
            'epoch': self.epoch,
            'model_state_dict': self.network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_dice': current_dice,
            'config': self.cfg
        }, self.save_dir / 'last.ckpt')
        
        # save best
        if current_dice > self.best_metric:
            self.best_metric = current_dice
            self.patience_counter = 0
            
            torch.save({
                'epoch': self.epoch,
                'model_state_dict': self.network.state_dict(),
                'train_dice': current_dice,
            }, self.save_dir / 'best.ckpt')
            
            print(f"  New best model! Dice: {current_dice:.4f}")
        else:
            self.patience_counter += 1
    
    def _check_early_stopping(self, val_metrics: Dict[str, float]) -> bool:
        """check for early stopping"""
        patience = self.cfg['training'].get('patience', None)
        if patience and self.patience_counter >= patience:
            return True
        return False
    
    def predict(self, image: torch.Tensor) -> torch.Tensor:
        """predict on testing set"""
        self.network.eval()
        with torch.no_grad():
            image = image.to(self.device, dtype=torch.float32)
            pred = self.network(image)
            return torch.argmax(pred, dim=1)


if __name__ == "__main__":
    trainer = SegTrainer(r"D:\Research\Mathematics\SYSU_GBU\6_Lectures\0_MachineLearning\2_Final_homework\2_code\3d_unet_cfg.yaml")
    print(trainer.transforms)
    
    trainer.train_epoch()
    
    # self.dataloader_train = torch.utils.data.DataLoader()