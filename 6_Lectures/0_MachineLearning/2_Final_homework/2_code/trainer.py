"""
Original code is generated by **Kimi AI**

mofified by Ruijie HE according to code on website
- https://github.com/wolny/pytorch-3dunet/blob/master/pytorch3dunet/unet3d/trainer.py
"""

import importlib
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
from pathlib import Path
from typing import Dict, Any, Callable, Optional, TypeAlias
import torchvision
import yaml
import wandb
from tqdm import tqdm
import nibabel as nib
from collections import defaultdict
import matplotlib.pyplot as plt
from torch.amp import GradScaler, autocast_mode

import transforms
from transforms import Compose
from models import get_model
import losses
import datasets

NNModel: TypeAlias = torch.nn.Module | Callable[[torch.Tensor], torch.Tensor]

def filter_name(d: dict) -> dict:
    return {k:v for (k,v) in d.items() if "name" not in k}

class SegTrainer:
    """
    Universal Trainer Framework for 2D/3D Segmentation
    """
    
    def __init__(self, config_path: str):
        # read yaml file
    
        with open(config_path, 'r') as f:
            self.cfg = yaml.safe_load(f)
        
        # setting training device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"Using Device {self.device}")
        
        # training status 
        self.epoch = 0
        self.best_metric = 0.0
        self.patience_counter = 0
        
        # model checkpoint saving
        self.save_dir = Path(self.cfg['logging']['save_dir'])
        self.save_dir.mkdir(exist_ok=True)
        
        # logging
        if self.cfg['logging']['use_wandb']:
            wandb.init(project=self.cfg['logging']['project_name'], 
                      config=self.cfg, name=self.cfg['logging']['run_name'])
    
        # add neural network
        self.network: NNModel = get_model(self.cfg["model"])
        self.network = self.network.to(self.device)
        # optional weight initialization and deterministic seed
        def _init_weights(m):
            # Convs
            if isinstance(m, (torch.nn.Conv2d, torch.nn.Conv3d, torch.nn.ConvTranspose3d)):
                try:
                    torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                except Exception:
                    pass
                if getattr(m, 'bias', None) is not None:
                    torch.nn.init.constant_(m.bias, 0)
            # Linear
            elif isinstance(m, torch.nn.Linear):
                try:
                    torch.nn.init.xavier_normal_(m.weight)
                except Exception:
                    pass
                if getattr(m, 'bias', None) is not None:
                    torch.nn.init.constant_(m.bias, 0)
            # Batch/Instance/GroupNorm
            elif isinstance(m, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d,
                                torch.nn.InstanceNorm1d, torch.nn.InstanceNorm2d, torch.nn.InstanceNorm3d,
                                torch.nn.GroupNorm)):
                if getattr(m, 'weight', None) is not None:
                    torch.nn.init.constant_(m.weight, 1)
                if getattr(m, 'bias', None) is not None:
                    torch.nn.init.constant_(m.bias, 0)

        init_flag = self.cfg.get('training', {}).get('init_weights', True)
        if init_flag:
            seed = self.cfg.get('training', {}).get('seed', 42)
            if seed is not None:
                import numpy as _np
                torch.manual_seed(seed)
                _np.random.seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed_all(seed)
            self.network.apply(_init_weights)
            print("Applied weight initialization and seed" if seed is not None else "Applied weight initialization")
            
        print(f"Network Params: {sum(p.numel() for p in self.network.parameters()):,}")
    
        self.optimizer: torch.optim.Optimizer = torch.optim.AdamW(**self.cfg["optimizer"], params=self.network.parameters())
    
        self.scheduler: torch.optim.lr_scheduler.LRScheduler = getattr(torch.optim.lr_scheduler, self.cfg["lr_scheduler"]["name"])(**filter_name(self.cfg["lr_scheduler"]), optimizer = self.optimizer) 
        
        self.loss_fn = getattr(losses, self.cfg["loss"]["name"])(**filter_name(self.cfg["loss"]))

        # setting up datasets
        self.loaders_cfg = self.cfg["loaders"]
        self.transforms = {
            "train": self.get_transform(self.loaders_cfg["train"]["transformer"]),
            "val": self.get_transform(self.loaders_cfg["val"]["transformer"]),
        }
        self.dataset_cfg = self.loaders_cfg["dataset"]
        self.dataset_train: Dataset = getattr(datasets, self.dataset_cfg["name"])(**filter_name(self.dataset_cfg), 
                                                                         mode="train", 
                                                                         transform=self.transforms["train"],
                                                                         slice_builder_config=self.loaders_cfg["train"]["slice_builder"]
                                                                         )
        self.dataset_val: Dataset = getattr(datasets, self.dataset_cfg["name"])(**filter_name(self.dataset_cfg), 
                                                                         mode="val", 
                                                                         transform=self.transforms["val"],
                                                                         slice_builder_config=self.loaders_cfg["train"]["slice_builder"]
                                                                         )
        
        self.dataloader_train = DataLoader(self.dataset_train, 
                                           batch_size=self.loaders_cfg["batch_size"], 
                                           shuffle=True,
                                           pin_memory=True, 
                                           num_workers=self.loaders_cfg["num_workers"], 
                                           persistent_workers=True,
                                           prefetch_factor=self.loaders_cfg["num_workers"]//2)
        
        self.dataloader_val = DataLoader(self.dataset_val, 
                                           batch_size=2 * self.loaders_cfg["batch_size"], 
                                           shuffle=False,
                                           pin_memory=True, 
                                           num_workers=self.loaders_cfg["num_workers"]//2, 
                                           prefetch_factor=self.loaders_cfg["num_workers"]//4)
        
    def get_transform(self, cfg: dict):
        """
        parse yaml config transform to transform for train loader and valid loader. 
        """
        
        raw_tf = {}
        
        for (k,v) in cfg.items():
            transforms_ls = []
            
            for tr in v:
                if len(tr) == 1:
                    transforms_ls.append(
                        getattr(transforms, tr["name"])()
                    )
                else:
                    transforms_ls.append(
                        getattr(transforms, tr["name"])(**filter_name(tr))
                    )
                
            tf = Compose(transforms=transforms_ls)
            raw_tf[k] = tf
           
        return raw_tf
    
    def train_epoch(self, scalar: GradScaler) -> Dict[str, float]:
        """train ONE epoch"""
        self.network.train()
        metrics = defaultdict(list)
        
        pbar = tqdm(enumerate(self.dataloader_train), desc=f"Train Epoch {self.epoch}",
                    total=len(iter(self.dataloader_train)))
        
        # loss_sum = 0
        # 
        for idx, batch in pbar:
            # move training data and label to device
            image = batch[0].to(self.device, dtype=torch.float32)  # (B, 1, D, H, W)
            # image = batch[0].to(self.device, dtype=torch.float32)  # (B, 1, D, H, W)
            target = self.soft(batch[1].to(self.device, dtype=torch.float32))    # (B, D, H, W)
            
            target, _ = target[:, :1, ...], target[:, 1:, ...]
            
            # prediction
            self.optimizer.zero_grad()
            
            with torch.autocast(device_type="cuda"):
                pred, logits = self.network(image, return_logits=True)  # output shape same as input

                # print(pred.max().item(), pred.min().item(), logits.max().item(), logits.min().item())
                # compute loss
                loss, bce, dice = self.loss_fn(pred, logits, target)

            # loss_sum += loss.detach()
            # backward pass
            loss.backward()
            
            if 'grad_clip' in self.cfg['training']:
                torch.nn.utils.clip_grad_norm_(
                    self.network.parameters(), 
                    self.cfg['training']['grad_clip']
                )
                
            self.optimizer.step()
            
            # show mean loss to reduce number of .item() calls
            metrics['loss'].append(loss.detach().item())
            
            # calculate gradient norm
            grad_norm_sum = sum(p.grad.norm() for p in self.network.parameters() if p.grad is not None)
            
            pbar.set_postfix({
                                'total_loss': f"{loss.detach().item():.3f}",
                                'bce': f"{bce.detach().item():.3f}",
                                'dice': f"{dice.detach().item():.3f}",
                                'grad_norm': f"{grad_norm_sum.detach().item():.3f}"
                            })
            
        
        return {k: np.mean(v) for k, v in metrics.items()}
    
    def soft(self, l, e=1e-2):
        return torch.clamp(l, e, 1-e)
    
    @torch.no_grad()
    def validate(self) -> Dict[str, float]:
        """
        test or validate on validation set
        """
        self.network.eval()
        metrics = defaultdict(list)
        
        with torch.no_grad():
            pbar = tqdm(self.dataloader_val, desc=f"Val Epoch {self.epoch}")
            
            for batch in pbar:
                image = batch[0].to(self.device, dtype=torch.float32)
                # image = batch[0].to(self.device, dtype=torch.float32)
                target = self.soft(batch[1].to(self.device, dtype=torch.float32))
                
                target, target_border = target[:, :1, ...], target[:, 1:, ...]
                
                with torch.autocast(device_type="cuda"):
                    # forward pass`
                    pred, logits = self.network(image, return_logits=True)
                    
                    # compute loss
                    loss, bce, dice = self.loss_fn(pred, logits, target)
                
                assert torch.isnan(loss).sum() == 0, print(loss, pred, logits, target, sep='\n', flush=True)
                 
                metrics['val_loss'].append(loss.item())
                metrics['val_dice'].append(dice.item())
                pbar.set_postfix(
                    {
                        'val_loss': f"{loss.item():.3f}", 
                        'val_dice': f"{dice.item():.3f}"
                    }
                )
        
        return {k: np.mean(v) for k, v in metrics.items()}
    
    def fit(self):
        """
        main training loop
        """
        
        torch.backends.cudnn.benchmark = True
        
        print(f"Starting training [{self.cfg['training']['max_num_epochs']}] epochs")
        
        scaler = GradScaler()

        for epoch in range(self.cfg['training']['max_num_epochs']):
            self.epoch = epoch
            
            # train ONE epoch
            train_metrics = self.train_epoch(scaler)
            # train_metrics = {"loss": 0}
            
            # validation
            val_metrics = self.validate()
            # val_metrics = {"val_loss": 0}
            
            # update LR scheduler
            if self.scheduler:
                self.scheduler.step(val_metrics['val_loss'])
            
            # add logging info
            self._log_metrics({**train_metrics, 
                               **val_metrics
                               })
            
            # save checkpoints
            self._save_checkpoint(val_metrics)
            
            # Early stopping
            if self._check_early_stopping(val_metrics):
                print(f"Early stopping triggered at epoch {epoch}")
    
    def _log_metrics(self, metrics: Dict[str, float]):
        """
        logging
        """
        print(f"Epoch {self.epoch}: " + 
              f"Train Loss: {metrics['loss']:.4f}, " +
              f"Val Loss: {metrics['val_loss']:.4f}")
        
        if self.cfg['logging']['use_wandb']:
            wandb.log(metrics, step=self.epoch)
    
    def _save_checkpoint(self, val_metrics: Dict[str, float]):
        """
        save latest & best model
        """
        current_dice = 1 - val_metrics['val_dice'] * 5 / 4
        
        # save latest
        torch.save({
            'epoch': self.epoch,
            'model_state_dict': self.network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_dice': current_dice,
            'config': self.cfg
        }, self.save_dir / 'last.ckpt')
        
        # save best
        if current_dice > self.best_metric:
            self.best_metric = current_dice
            self.patience_counter = 0
            
            torch.save({
                'epoch': self.epoch,
                'model_state_dict': self.network.state_dict(),
                'train_dice': current_dice,
            }, self.save_dir / 'best.ckpt')
            
            print(f"  New best model! Dice: {current_dice:.4f}")
        else:
            self.patience_counter += 1
    
    def _check_early_stopping(self, val_metrics: Dict[str, float]) -> bool:
        """check for early stopping"""
        patience = self.cfg['training'].get('patience', None)
        if patience and self.patience_counter >= patience:
            return True
        return False
    
    def predict(self, image: torch.Tensor) -> torch.Tensor:
        """predict on testing set"""
        self.network.eval()
        with torch.no_grad():
            image = image.to(self.device, dtype=torch.float32)
            pred = self.network(image)
            return torch.argmax(pred, dim=1)


if __name__ == "__main__":
    from pyinstrument import Profiler
    
    torch.random.manual_seed(42)
    
    profiler = Profiler()
    profiler.start()
    
    trainer = SegTrainer(r"D:\Research\Mathematics\SYSU_GBU\6_Lectures\0_MachineLearning\2_Final_homework\2_code\3d_unet_cfg.yaml")
    print(trainer.transforms)
    
    trainer.fit()
    
    profiler.stop()
    
    with open('profile.html', 'w') as f: 
        f.write(profiler.output_html())
    
    # self.dataloader_train = torch.utils.data.DataLoader()